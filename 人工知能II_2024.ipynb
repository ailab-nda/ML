{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/2InxqcCSZy95zcH3M65U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/ML/blob/main/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BDII_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 人工知能II試験問題\n",
        "\n",
        "以下の問題について解答し、結果を提出してください。なお、解答については文章はテキストセルに、プログラムに関してはコードセルに記入すること。また、結果の提出については、この画面の左上の「ファイル」メニュー内にある「ダウンロード」->「.ipynbをダウンロード」で自分のマシンにダウンロードできるので、それを各自の学内メールに添付してください。\n",
        "\n",
        "数式の書き方 --> https://qiita.com/PlanetMeron/items/63ac58898541cbe81ada"
      ],
      "metadata": {
        "id": "6cmZnSwSLFS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 用語説明"
      ],
      "metadata": {
        "id": "EMP3Omloc1_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " (1) ニューラルネットワークにおける勾配消失問題とはどのようなものか？またそれはどのような時に生じるか？"
      ],
      "metadata": {
        "id": "waEhIMDbdcqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "（解答欄）\n"
      ],
      "metadata": {
        "id": "0DCRduwRdeMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) 5x5 の入力データ $X$ に、1x2のカーネル $k$ を用いて畳み込み操作を行ったときの結果を記せ。ただし、\n",
        "\n",
        "$\n",
        "x = \\left[\n",
        "    \\begin{matrix}\n",
        "    1 & 1 & 0 & 1 & 0 \\\\\n",
        "    0 & 0 & 0 & 1 & 0 \\\\\n",
        "    1 & 1 & 1 & 1 & 1 \\\\\n",
        "    1 & 0 & 1 & 0 & 1 \\\\\n",
        "    0 & 0 & 0 & 1& 1\n",
        "    \\end{matrix}\n",
        "    \\right],\n",
        "$\n",
        "$\n",
        "k = \\left[\n",
        "    \\begin{matrix}\n",
        "    1 & -1\n",
        "    \\end{matrix}\n",
        "    \\right]\n",
        "$\n",
        "\n",
        "とし、ストライドは 1 とする。"
      ],
      "metadata": {
        "id": "AWzVXsZjdjJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "（解答欄）\n"
      ],
      "metadata": {
        "id": "Fljviq12gE1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 数式とプログラム"
      ],
      "metadata": {
        "id": "ODyLb4xYbntN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Transformer における Scaled Dot-Product Attention を数式で表せ。"
      ],
      "metadata": {
        "id": "YLIIJrSYnOqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【解答欄】"
      ],
      "metadata": {
        "id": "KZjkG9Kjgah7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) 以下は Scaled Dot-Product Attion の計算例である。このプログむを読んで以下の問に答えよ。"
      ],
      "metadata": {
        "id": "77dp7gAXnRBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# sentence = \"I love NDA\"\n",
        "token_ids  = [0,  1,  2]\n",
        "token_ids = torch.tensor(token_ids, dtype=torch.int64)\n",
        "\n",
        "vocab_size = 1000  # assume the number of unique vocabularies\n",
        "d = 32       # dimensionality of feature vectors\n",
        "\n",
        "embedding = torch.nn.Embedding(vocab_size, d)\n",
        "X = embedding(token_ids)\n",
        "\n",
        "W_q = torch.randn(d_model, d)\n",
        "W_k = torch.randn(d_model, d)\n",
        "W_v = torch.randn(d_model, d)\n",
        "\n",
        "Q = X.matmul(W_q)\n",
        "K = X.matmul(W_k)\n",
        "V = X.matmul(W_v)\n",
        "\n",
        "# 以下に Scaled Dot-Product Attention の計算を実装せよ\n",
        "\n",
        "attention = 0 # エラーにならないように仮に０としている\n",
        "print(attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN3z2_DGZfJl",
        "outputId": "657fefb4-f3f6-412b-e7fc-d3c322f08711"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) $Q, K, V$ の大きさ（次元）はいくつか？"
      ],
      "metadata": {
        "id": "RXaCSqUdd_gW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【解答欄】\n"
      ],
      "metadata": {
        "id": "GsNAWVVjs2Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) 上記のプログラムにおいて $Q, K, V$ を使った Scaled Dot-Product Attention を実装せよ。"
      ],
      "metadata": {
        "id": "eKGoKUcXeQp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 大規模言語モデル"
      ],
      "metadata": {
        "id": "qL0toXT4gh5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "２つ以上の大規模言語モデルを使用し、使用感を比較せよ。使用する大規模言語モデルは、複数メーカのものでもいいし、同一メーカーの別モデルでも構わない。また、試す課題（執筆、要約、数学、など）についても自由とする。"
      ],
      "metadata": {
        "id": "bMY4ZhbzTMe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【解答欄】"
      ],
      "metadata": {
        "id": "OqNO6JVXhJxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 自由記述"
      ],
      "metadata": {
        "id": "tlJcI_mGpRmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "（解答欄）\n"
      ],
      "metadata": {
        "id": "KZg-hp1LpZno"
      }
    }
  ]
}