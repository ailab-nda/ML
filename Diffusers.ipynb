{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/ML/blob/main/Diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lchD3PeVx8MP"
      },
      "outputs": [],
      "source": [
        "#stable diffusionのインストール\n",
        "!pip install --upgrade diffusers[torch] transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dGvacOuuyGg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install safetensors\n",
        "\n",
        "import torch\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "\n",
        "def load_safetensors_lora(pipeline, checkpoint_path, LORA_PREFIX_UNET=\"lora_unet\", LORA_PREFIX_TEXT_ENCODER=\"lora_te\", alpha=0.75):\n",
        "    # load LoRA weight from .safetensors\n",
        "    state_dict = load_file(checkpoint_path)\n",
        "\n",
        "    visited = []\n",
        "\n",
        "    # directly update weight in diffusers model\n",
        "    for key in state_dict:\n",
        "        # it is suggested to print out the key, it usually will be something like below\n",
        "        # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n",
        "\n",
        "        # as we have set the alpha beforehand, so just skip\n",
        "        if \".alpha\" in key or key in visited:\n",
        "            continue\n",
        "\n",
        "        if \"text\" in key:\n",
        "            layer_infos = key.split(\".\")[0].split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n",
        "            curr_layer = pipeline.text_encoder\n",
        "        else:\n",
        "            layer_infos = key.split(\".\")[0].split(LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n",
        "            curr_layer = pipeline.unet\n",
        "\n",
        "        # find the target layer\n",
        "        temp_name = layer_infos.pop(0)\n",
        "        while len(layer_infos) > -1:\n",
        "            try:\n",
        "                curr_layer = curr_layer.__getattr__(temp_name)\n",
        "                if len(layer_infos) > 0:\n",
        "                    temp_name = layer_infos.pop(0)\n",
        "                elif len(layer_infos) == 0:\n",
        "                    break\n",
        "            except Exception:\n",
        "                if len(temp_name) > 0:\n",
        "                    temp_name += \"_\" + layer_infos.pop(0)\n",
        "                else:\n",
        "                    temp_name = layer_infos.pop(0)\n",
        "\n",
        "        pair_keys = []\n",
        "        if \"lora_down\" in key:\n",
        "            pair_keys.append(key.replace(\"lora_down\", \"lora_up\"))\n",
        "            pair_keys.append(key)\n",
        "        else:\n",
        "            pair_keys.append(key)\n",
        "            pair_keys.append(key.replace(\"lora_up\", \"lora_down\"))\n",
        "\n",
        "        # update weight\n",
        "        if len(state_dict[pair_keys[0]].shape) == 4:\n",
        "            weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).to(torch.float32)\n",
        "            weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).to(torch.float32)\n",
        "            curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n",
        "        else:\n",
        "            weight_up = state_dict[pair_keys[0]].to(torch.float32)\n",
        "            weight_down = state_dict[pair_keys[1]].to(torch.float32)\n",
        "            curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)\n",
        "\n",
        "        # update visited list\n",
        "        for item in pair_keys:\n",
        "            visited.append(item)\n",
        "\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "7K7I9M3dyJ0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
        "from diffusers.models import AutoencoderKL\n",
        "import torch\n",
        "\n",
        "#画像生成に使うモデルデータ\n",
        "model_id = \"sinkinai/Beautiful-Realistic-Asians-v5\"#@param {type:\"string\"}\n",
        "#画像生成に使うVAE\n",
        "vae = \"stabilityai/sd-vae-ft-ema\"#@param {type:\"string\"}\n",
        "vae = AutoencoderKL.from_pretrained(vae)\n",
        "\n",
        "#画像生成に使うスケジューラー\n",
        "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "\n",
        "#パイプラインの作成\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, vae=vae, custom_pipeline=\"lpw_stable_diffusion\")\n",
        "\n",
        "#LoRAを読み込む\n",
        "#LoRA_USE = False #@param {type:\"boolean\"}\n",
        "#if LoRA_USE == True:\n",
        "#  LoRA=\"/content/drive/MyDrive/StableDiffusion/Lora/flat2.safetensors\"#@param {type:\"string\"}\n",
        "#  LoRA_alpha = -1#@param {type:\"number\"}\n",
        "#  pipe = load_safetensors_lora(pipe, LoRA, alpha=LoRA_alpha)\n",
        "\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "#NSFW規制を無効化する\n",
        "if pipe.safety_checker is not None:\n",
        "  pipe.safety_checker = lambda images, **kwargs: (images, False)"
      ],
      "metadata": {
        "id": "qSl5PrmMyjhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import os\n",
        "import random\n",
        "import IPython\n",
        "from IPython.display import Image\n",
        "\n",
        "#txt2img出力画像の保存先\n",
        "!mkdir -p /content/txt2img_output\n",
        "\n",
        "# ファイル名に使う日付と時刻のフォーマットを定義する\n",
        "file_format = \"%Y%m%d_%H%M%S\"\n",
        "i=0\n",
        "\n",
        "#ポジティブプロンプト\n",
        "prompt = \"japanese, girl, Tokyo street, night, cityscape, city lights, upper body, close-up, 8k, RAW photo, best quality, masterpiece, realistic, photo-realistic\"#@param {type:\"string\"}\n",
        "#ネガティブプロンプト\n",
        "n_prompt = \"EasyNegativeV2,negative_hand-neg, (worst quality:2), (low quality:2), (normal quality:2), lowres, bad anatomy, bad hands, normal quality, ((monochrome)), ((grayscale))\"#@param {type:\"string\"}\n",
        "#CFG Scale\n",
        "CFG_scale = 7#@param {type:\"number\"}\n",
        "#ステップ数\n",
        "Steps = 20#@param {type:\"number\"}\n",
        "#seed値\n",
        "seed=-1#@param {type:\"number\"}\n",
        "if seed is None or seed == -1:\n",
        "  inputSeed = random.randint(0, 2147483647)\n",
        "else:\n",
        "  valueSeed = seed\n",
        "\n",
        "#生成枚数\n",
        "num_images = 3#@param {type:\"number\"}\n",
        "#出力画像の横幅\n",
        "width = 768#@param {type:\"number\"}\n",
        "#出力画像の高さ\n",
        "height = 512#@param {type:\"number\"}\n",
        "#出力画像を保存するフォルダ\n",
        "save_path = \"/content/txt2img_output\"#@param {type:\"string\"}\n",
        "\n",
        "while i < int(num_images):\n",
        "  #generator\n",
        "  if seed is None or seed == -1:valueSeed = inputSeed + i\n",
        "  generator = torch.Generator(device=\"cuda\").manual_seed(valueSeed)\n",
        "\n",
        "  #画像を生成\n",
        "  image = pipe(prompt, negative_prompt=n_prompt, width=width, height=height, generator=generator, guidance_scale=CFG_scale, num_inference_steps=Steps).images[0]\n",
        "\n",
        "  # 現在の日本時間を取得\n",
        "  jst_now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))\n",
        "  #出力する画像の名前を生成する\n",
        "  file_name = (jst_now.strftime(file_format)+ \"_\" + str(valueSeed))\n",
        "  image_name = file_name + f\".png\"\n",
        "\n",
        "  #画像を保存する\n",
        "  save_location = os.path.join(save_path, image_name)\n",
        "  image.save(save_location)\n",
        "  IPython.display.display(IPython.display.Image(save_location))\n",
        "  i = i + 1"
      ],
      "metadata": {
        "id": "yoxNPly0yqY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ファイルをDL\n",
        "from google.colab import files\n",
        "import shutil\n",
        "# フォルダをzip圧縮\n",
        "shutil.make_archive(\"txt2img_output\", \"zip\", \"/content/txt2img_output\")\n",
        "# 圧縮ファイルをダウンロード\n",
        "files.download(\"txt2img_output.zip\")"
      ],
      "metadata": {
        "id": "7uCXaTK0zDgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}