{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習III 第１回 Web スクレイピング\n",
    "\n",
    "出典　https://code.dividable.net/tutorials/python-scraping-blog/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. モジュールのインストールとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  --trusted-host pypi.org --trusted-host files.pythonhosted.org beautifulsoup4 japanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BeautifulSoup の初期化\n",
    "\n",
    "BeautifulSoupで、HTMLを読み込みます。本来は、外部サイトのHTMLを取得して、そのデータを受け取りますが、学習のため、こちらで用意したHTMLを利用してみたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# こちらで用意したHTML\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "# BeautifulSoupの初期化\n",
    "soup = BeautifulSoup(html_doc, 'html.parser') # BeautifulSoupの初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()) # HTMLをインデントすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BeautifulSoupでtitleを取得\n",
    "\n",
    "BeautifulSoupを利用する方法はいたってシンプルです。\n",
    "\n",
    "titleタグの内容を取りたい場合は、soup.titleで取得できます。\n",
    "\n",
    "例： \\<title\\>The Dormouse’s story\\</title\\>\n",
    "\n",
    " titleタグの中身の文字だけを取りたい場合は、soup.title.stringで取得できます。\n",
    " \n",
    "例：The Dormouse’s story\n",
    "\n",
    "さっそく試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <title>The Dormouse's story</title>を取得してください。\n",
    "print(soup.title)\n",
    "# The Dormouse's story を取得してください。\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BeautifulSoupで複数のaタグを取得\n",
    "\n",
    "では、今度は同じタグが複数存在する場合も見てみましょう。現状、aタグは3つあります。これらのタグをすべて取りたいです。そこで、このaタグを取ろうとして、soup.aを試すと、最初の一つだけとることになり、すべてを取ることができません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで、今回は複数のタグを取る find_all メソッドを利用して、複数タグを取得します。 soup.find_all(‘a’)のように指定すると、リスト形式ですべてのaタグを取得することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、取得したaタグのリンクを一つ一つ取り出してみましょう。リストの中から、一つ一つaタグを取得していきます。\n",
    "\n",
    "## TODO\n",
    "1. for文を利用して、取得したaタグのHTMLを含む部分をprintしてください。\n",
    "2. for文を利用して、取得したaタグのHTMLを含まない部分だけprintしてください。\n",
    "ヒント\n",
    "1. リスト形式のデータは、for 単数 in リスト 構文を利用して、一つひとつ取り出すことができます。\n",
    "2. HTMLを含まない中身をとるものは、stringを利用するのでした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.find_all(\"a\")\n",
    "# for文を利用して、取得したaタグのHTMLを含む部分をprintしてください。\n",
    "print(\"1. \")\n",
    "for tag in tags:\n",
    "   print (tag)\n",
    "# for文を利用して、取得したaタグのHTMLを含む部分をprintしてください。\n",
    "print(\"2. \")\n",
    "for tag in tags:\n",
    "   print (tag.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BeautifulSoupでURLの取得\n",
    "\n",
    "今度は、取得したaタグのhrefに当たる部分、つまりURLだけとりだしてみましょう。soup.a.get(“href”)のように指定することで、URLを取得することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.get(\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、さきほど取得したaタグのすべてのURLをprintしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1 for文を利用して、aタグのURLをすべてprintしてください\n",
    "for link in tags:\n",
    " print (link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際のサイトをスクレイピングしてみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requests: WebページのHTMLを取得しよう\n",
    "下記のページから、記事のタイトルと、その記事のURLを取得してみましょう。実際にWebページからデータを取得するのは、requestsというライブラリを利用します。 requestsをimportします。requests.get(url)でurlのページの情報を取得し、textを実行するとHTMLの内容を取得することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://review-of-my-life.blogspot.com/\", verify='FG_CA.cer')\n",
    "print (response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = response.text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser') # BeautifulSoupの初期化\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas: CSVにデータを保存しよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- 記事名と記事URLをすべて取得し、CSVに出力してください。\n",
    "- データフレームを作成してください。列名は、name, urlです。\n",
    "- 記事名と記事URLをデータフレームに追加してください\n",
    "- result.csvという名前でCSVに出力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記事名と記事URLの取得\n",
    "tags = soup.find_all(\"h3\",{\"class\":\"post-title\"})\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームを作成してください。列名は、name, urlです。\n",
    "columns = [\"name\", \"url\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "# 記事名と記事URLをデータフレームに追加してください\n",
    "for tag in tags:\n",
    " name = tag.a.string\n",
    " url = tag.a.get(\"href\")\n",
    " se = pd.Series([name, url], columns)\n",
    " print(se)\n",
    " df = df.append(se, columns)\n",
    "# result.csvという名前でCSVに出力してください。\n",
    "filename = \"result.csv\"\n",
    "df.to_csv(filename, encoding = 'SHIFT-JIS', index=False) #encoding指定しないと、エラーが起こります。おまじないだともって入力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題\n",
    "売れているノートパソコンは大体いくら位のものか？スクレイピングを利用して調べよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dcf9e2717909001467cf879b1089e6e80dddbfb5060f305c8aadf7a9adff89a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
