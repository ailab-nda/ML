{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8e6629-408c-4672-bb5c-63c3f611174b",
   "metadata": {},
   "source": [
    "# 演習III 第４回 ツイッターテキスト解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a68669-906f-4364-a00e-53226a5ebf90",
   "metadata": {},
   "source": [
    "出典：https://qiita.com/e10persona/items/7a7643b266c2bdfbf7d0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b15b3",
   "metadata": {},
   "source": [
    "Google Colab 用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install aptitude\n",
    "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
    "!pip install mecab-python3==0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f7ddb-42c6-4081-b7a2-859d5e02823d",
   "metadata": {},
   "source": [
    "必要なライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98732b9-1901-4093-aae0-fb2e37ae04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy mecab-python3 wordcloud oseti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d13133",
   "metadata": {},
   "source": [
    "証明書関係エラー防止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a38a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ea599-3e8a-454b-8ac3-f3e3d3cf9c2d",
   "metadata": {},
   "source": [
    "ライブラリのインポートを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51196983-f9a7-4964-9a9a-1b736f240b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import MeCab\n",
    "import csv\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import oseti\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bade572-79dd-464c-89fc-deba2c872af2",
   "metadata": {},
   "source": [
    "## MeCab のテスト"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c48559-4684-49e7-9113-c6d769840e4b",
   "metadata": {},
   "source": [
    "※ MeCab は別途インストールしておくこと"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e91e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = MeCab.Tagger(\"-Ochasen\") \n",
    "malist = mecab.parse(\"すもももももももものうち\")\n",
    "print(malist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209c15c",
   "metadata": {},
   "source": [
    "## Tweet の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45afdd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config # config.py\n",
    "CK = config.CONSUMER_KEY\n",
    "CS = config.CONSUMER_SECRET\n",
    "AT = config.ACCESS_TOKEN\n",
    "AS = config.ACCESS_TOKEN_SECRET\n",
    "BT = config.BEARER_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ae03d-9503-42d9-a21c-c252c25829c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token=BT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92118ff",
   "metadata": {},
   "source": [
    "## 特定ユーザーのツイートの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = client.get_users_tweets(id=id , tweet_fields=['context_annotations', 'created_at'], max_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'ndanyusi: 防衛大学校入試広報（公式）\n",
    "user = client.get_user(username='ndanyusi')\n",
    "id = user.data.id\n",
    "\n",
    "tweets = client.get_users_tweets(id=id , tweet_fields=['context_annotations', 'created_at'], max_results=100)\n",
    "for tweet in tweets.data:\n",
    "    print(user.data.name)\n",
    "    print(tweet.created_at)\n",
    "    print(tweet.text)\n",
    "    print(\"=================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb44d0",
   "metadata": {},
   "source": [
    "### キーワードを含むツイートの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own search query\n",
    "query = '防衛大 -is:retweet'\n",
    "\n",
    "results = []\n",
    "# Replace the limit=10 with the maximum number of Tweets you want\n",
    "for tweet in tweepy.Paginator(client.search_recent_tweets, query=query,\n",
    "                              tweet_fields=['context_annotations', 'created_at'], user_fields=['username'],expansions=['author_id'], max_results=100).flatten(limit=100):\n",
    "    username = client.get_user(id=tweet.data['author_id']).data.name\n",
    "    dat = [username, tweet.created_at, tweet.text]\n",
    "    print(dat)\n",
    "    results.append(dat)\n",
    "    print(\"=================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9932af",
   "metadata": {},
   "source": [
    "## 結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=['ユーザ名', '日時', 'テキスト'])\n",
    "df.to_csv('tweets.csv', index=False, encoding='shift-jis', errors='ignore')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2dc6f9",
   "metadata": {},
   "source": [
    "## 結果の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90110c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('tweets.csv', encoding='cp932')\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac63c7",
   "metadata": {},
   "source": [
    "## MeCab で分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0e4f6",
   "metadata": {},
   "source": [
    "### 分かち書き、品詞の抜き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for tweet in df['テキスト']:\n",
    "    node = mecab.parseToNode(tweet)\n",
    "    while node:\n",
    "        word = node.surface\n",
    "        word_type = node.feature.split(\",\")[0]\n",
    " \n",
    "        # \"名詞\", \"動詞\", \"形容詞\", \"副詞\"の中で選択したものを抽出\n",
    "        if word_type in [\"名詞\", \"動詞\", \"形容詞\"]:\n",
    "            words.append(word)\n",
    "        node = node.next\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbc26b",
   "metadata": {},
   "source": [
    "### WordCloud として表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloudで出力するフォントを指定\n",
    "font_path = \"/System/Library/Fonts/ヒラギノ角ゴシック W4.ttc\"\n",
    "txt = \" \".join(words)\n",
    "\n",
    "# ストップワードの設定(特定のワードは入らないように設定)\n",
    "stop_words = ['https', 't', 'co',  'RT']\n",
    "\n",
    "#解析した単語、ストップワードを設定、背景の色は白\n",
    "wordcloud = WordCloud(background_color=\"white\", font_path=font_path, stopwords=set(stop_words),\n",
    "    width=800,height=600).generate(txt)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802816e5",
   "metadata": {},
   "source": [
    "## ポジ・ネガ分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = oseti.Analyzer()\n",
    "print(analyzer.analyze_detail(\"最後まで希望を捨てちゃいかん。あきらめたら、そこで試合終了だよ。\"))\n",
    "print(analyzer.analyze_detail(\"認めたくないものだな。自分自身の、若さゆえの過ちというものを。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_senti = []\n",
    "for tweet in df['テキスト']:\n",
    "    print(tweet)\n",
    "    senti = analyzer.analyze(tweet)\n",
    "    print(analyzer.analyze_detail(tweet), np.mean(senti))\n",
    "    ave_senti.append(np.mean(senti))\n",
    "ave_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(ave_senti, bins=4)\n",
    "plt.rcParams['font.family'] = \"AppleGothic\"\n",
    "plt.pie(hist, labels=['ネガ', 'ややネガ', 'ややポジ', 'ポジ'], counterclock=False, startangle=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e175c",
   "metadata": {},
   "source": [
    "## 感情分析（Transformer 版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers fugashi ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "from transformers import AutoModelForSequenceClassification \n",
    "from transformers import BertJapaneseTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('daigo/bert-base-japanese-sentiment') \n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking') \n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp(\"最後まで希望を捨てちゃいかん。あきらめたら、そこで試合終了だよ。\"))\n",
    "print(nlp(\"認めたくないものだな。自分自身の、若さゆえの過ちというものを。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8931c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "for tweet in df['テキスト']:\n",
    "    print(tweet)\n",
    "    senti = nlp(tweet)\n",
    "    print(senti)\n",
    "    sentiments.append(senti[0]['score'])\n",
    "print(sentiments)\n",
    "print(np.mean(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(sentiments, bins=4)\n",
    "plt.rcParams['font.family'] = \"AppleGothic\"\n",
    "plt.pie(hist, labels=['ネガ', 'ややネガ', 'ややポジ', 'ポジ'], counterclock=False, startangle=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53517b1d",
   "metadata": {},
   "source": [
    "## 課題\n",
    "各自のテーマで Tweet を分析せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6069d67a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6b05b93daf3f9d51f6f9bad260edf5badcb96fbb82eb040035f228177e317a6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
