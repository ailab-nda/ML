{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "7_1_6_audio_preprocessing_tutorial_jp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/NLP/blob/main/7_1_6_audio_preprocessing_tutorial_jp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLB50Osb3iJL"
      },
      "source": [
        "「torchaudioによる音声データの取り扱い」\n",
        "======\n",
        "【原題】AUDIO MANIPULATION WITH TORCHAUDIO\n",
        "\n",
        "【元URL】https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html\n",
        "\n",
        "【翻訳】電通国際情報サービスISID AIトランスフォーメーションセンター　大串 和正\n",
        "\n",
        "【日付】2021年4月17日\n",
        "\n",
        "【チュトーリアル概要】<br>\n",
        "``torchaudio``を使い、音声データの基本的な取り扱い方法を説明します。\n",
        "\n",
        "英語版のチュートリアルサイトでは項目は分かれているがJupyter Notebookは同じである、\n",
        "\n",
        "- 7.1 torchaudioによる音声データの取り扱い\n",
        "- 7.2 音声データの入出力（AUDIO I/O）\n",
        "- 7.3 データオーギュメンテーション（DATA AUGMENTATION）\n",
        "- 7.4 特徴量抽出（FEATURE EXTRACTIONS）\t\n",
        "- 7.5 特徴量オーギュメンテーション（FEATURE AUGMENTATION）\n",
        "- 7.6 データセット（DATASETS）\n",
        "\n",
        "を本ページで解説します。\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wrIkRI0j1zz"
      },
      "source": [
        "7.1 torchaudioによる音声データの取り扱い\n",
        "==================================\n",
        "\n",
        "``torchaudio`` には強力な音声データの入出力関数、前処理変換、データセットが実装されています。\n",
        "\n",
        "本チュートリアルでは音声データの準備方法とニューラルネットワークモデルへ入力可能な特徴量の抽出方法を解説します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlWGbkcFj1zu"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4aBgyNKj1zz"
      },
      "source": [
        "# Google Colabでこのチュートリアルを実行するには、\n",
        "# 次のコマンドで必要なパッケージをインストールします\n",
        "!pip install torchaudio librosa boto3\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHaBm_P1j1zz"
      },
      "source": [
        "データとユーティリティ関数の準備 (この節はセルを実行するだけです)\n",
        "--------------------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHyaRtjZj1z0"
      },
      "source": [
        "#@title データとユーティリティ関数の準備 {display-mode: \"form\"}\n",
        "#@markdown\n",
        "#@markdown このセルの中身を確認する必要はありません。\n",
        "#@markdown\n",
        "#@markdown **本セルを一度実行して、次のセルへと進んでください**\n",
        "#@markdown\n",
        "#@markdown このチュートリアルでは [VOiCES dataset](https://iqtlabs.github.io/voices/) のデータセットを使用します。ライセンスは Creative Commons BY 4.0 です。\n",
        "#@markdown\n",
        "#@markdown 　\n",
        "\n",
        "#@markdown ※ **（日本語訳注）本セルをダブルクリックすると、非表示にしているプログラムが表示されます。**\n",
        "#@markdown\n",
        "#@markdown **本セルは「説明のテキスト・セル」ではなく、プログラムを非表示にしている「コードセル」なので、実行するのを忘れないように気をつけてください。** \n",
        "#@markdown\n",
        "#@markdown Google Clabで本セルの右上のメニュ（「…」が縦）→「フォーム」→「コードを表示」or「コードを非表示」でコードをセルに表示するかどうか設定できます。\n",
        "#@markdown\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Preparation of data and helper functions.\n",
        "#-------------------------------------------------------------------------------\n",
        "import io\n",
        "import os\n",
        "import math\n",
        "import tarfile\n",
        "import multiprocessing\n",
        "\n",
        "import scipy\n",
        "import librosa\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.config import Config\n",
        "import requests\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "[width, height] = matplotlib.rcParams['figure.figsize']\n",
        "if width < 10:\n",
        "  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n",
        "\n",
        "_SAMPLE_DIR = \"_sample_data\"\n",
        "SAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\n",
        "SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n",
        "\n",
        "SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
        "SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n",
        "\n",
        "SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n",
        "SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n",
        "\n",
        "SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n",
        "SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n",
        "\n",
        "SAMPLE_MP3_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.mp3\"\n",
        "SAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n",
        "\n",
        "SAMPLE_GSM_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.gsm\"\n",
        "SAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n",
        "\n",
        "SAMPLE_TAR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit.tar.gz\"\n",
        "SAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\n",
        "SAMPLE_TAR_ITEM = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
        "\n",
        "S3_BUCKET = \"pytorch-tutorial-assets\"\n",
        "S3_KEY = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
        "\n",
        "YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n",
        "os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(_SAMPLE_DIR, exist_ok=True)\n",
        "\n",
        "def _fetch_data():\n",
        "  uri = [\n",
        "    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n",
        "    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n",
        "    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n",
        "    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
        "    (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n",
        "    (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n",
        "    (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n",
        "  ]\n",
        "  for url, path in uri:\n",
        "    with open(path, 'wb') as file_:\n",
        "      file_.write(requests.get(url).content)\n",
        "\n",
        "_fetch_data()\n",
        "\n",
        "def _download_yesno():\n",
        "  if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n",
        "    return\n",
        "  torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n",
        "\n",
        "YESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\n",
        "YESNO_DOWNLOAD_PROCESS.start()\n",
        "\n",
        "def _get_sample(path, resample=None):\n",
        "  effects = [\n",
        "    [\"remix\", \"1\"]\n",
        "  ]\n",
        "  if resample:\n",
        "    effects.append([\"rate\", f'{resample}'])\n",
        "  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
        "\n",
        "def get_speech_sample(*, resample=None):\n",
        "  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n",
        "\n",
        "def get_sample(*, resample=None):\n",
        "  return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
        "\n",
        "def get_rir_sample(*, resample=None, processed=False):\n",
        "  rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n",
        "  if not processed:\n",
        "    return rir_raw, sample_rate\n",
        "  rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
        "  rir = rir / torch.norm(rir, p=2)\n",
        "  rir = torch.flip(rir, [1])\n",
        "  return rir, sample_rate\n",
        "\n",
        "def get_noise_sample(*, resample=None):\n",
        "  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n",
        "\n",
        "def print_metadata(metadata, src=None):\n",
        "  if src:\n",
        "    print(\"-\" * 10)\n",
        "    print(\"Source:\", src)\n",
        "    print(\"-\" * 10)\n",
        "  print(\" - sample_rate:\", metadata.sample_rate)\n",
        "  print(\" - num_channels:\", metadata.num_channels)\n",
        "  print(\" - num_frames:\", metadata.num_frames)\n",
        "  print(\" - bits_per_sample:\", metadata.bits_per_sample)\n",
        "  print(\" - encoding:\", metadata.encoding)\n",
        "  print()\n",
        "\n",
        "def print_stats(waveform, sample_rate=None, src=None):\n",
        "  if src:\n",
        "    print(\"-\" * 10)\n",
        "    print(\"Source:\", src)\n",
        "    print(\"-\" * 10)\n",
        "  if sample_rate:\n",
        "    print(\"Sample Rate:\", sample_rate)\n",
        "  print(\"Shape:\", tuple(waveform.shape))\n",
        "  print(\"Dtype:\", waveform.dtype)\n",
        "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
        "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
        "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
        "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
        "  print()\n",
        "  print(waveform)\n",
        "  print()\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "  for c in range(num_channels):\n",
        "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "    axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "      axes[c].set_ylim(ylim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "  for c in range(num_channels):\n",
        "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  if num_channels == 1:\n",
        "    display(Audio(waveform[0], rate=sample_rate))\n",
        "  elif num_channels == 2:\n",
        "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "  else:\n",
        "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
        "\n",
        "def inspect_file(path):\n",
        "  print(\"-\" * 10)\n",
        "  print(\"Source:\", path)\n",
        "  print(\"-\" * 10)\n",
        "  print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
        "  print_metadata(torchaudio.info(path))\n",
        "\n",
        "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title(title or 'Spectrogram (db)')\n",
        "  axs.set_ylabel(ylabel)\n",
        "  axs.set_xlabel('frame')\n",
        "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
        "  if xmax:\n",
        "    axs.set_xlim((0, xmax))\n",
        "  fig.colorbar(im, ax=axs)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def plot_mel_fbank(fbank, title=None):\n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title(title or 'Filter bank')\n",
        "  axs.imshow(fbank, aspect='auto')\n",
        "  axs.set_ylabel('frequency bin')\n",
        "  axs.set_xlabel('mel bin')\n",
        "  plt.show(block=False)\n",
        "\n",
        "def get_spectrogram(\n",
        "    n_fft = 400,\n",
        "    win_len = None,\n",
        "    hop_len = None,\n",
        "    power = 2.0,\n",
        "):\n",
        "  waveform, _ = get_speech_sample()\n",
        "  spectrogram = T.Spectrogram(\n",
        "      n_fft=n_fft,\n",
        "      win_length=win_len,\n",
        "      hop_length=hop_len,\n",
        "      center=True,\n",
        "      pad_mode=\"reflect\",\n",
        "      power=power,\n",
        "  )\n",
        "  return spectrogram(waveform)\n",
        "\n",
        "def plot_pitch(waveform, sample_rate, pitch):\n",
        "  figure, axis = plt.subplots(1, 1)\n",
        "  axis.set_title(\"Pitch Feature\")\n",
        "  axis.grid(True)\n",
        "\n",
        "  end_time = waveform.shape[1] / sample_rate\n",
        "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
        "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
        "\n",
        "  axis2 = axis.twinx()\n",
        "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
        "  ln2 = axis2.plot(\n",
        "      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
        "\n",
        "  axis2.legend(loc=0)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n",
        "  figure, axis = plt.subplots(1, 1)\n",
        "  axis.set_title(\"Kaldi Pitch Feature\")\n",
        "  axis.grid(True)\n",
        "\n",
        "  end_time = waveform.shape[1] / sample_rate\n",
        "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
        "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
        "\n",
        "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
        "  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
        "  axis.set_ylim((-1.3, 1.3))\n",
        "\n",
        "  axis2 = axis.twinx()\n",
        "  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n",
        "  ln2 = axis2.plot(\n",
        "      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n",
        "\n",
        "  lns = ln1 + ln2\n",
        "  labels = [l.get_label() for l in lns]\n",
        "  axis.legend(lns, labels, loc=0)\n",
        "  plt.show(block=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRRIFo1j1z3"
      },
      "source": [
        "7.2 音声データの入出力（AUDIO I/O）\n",
        "=========\n",
        "\n",
        "torchaudio には ``libsox`` が統合されており、様々な音声データの入出力が可能です。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hezT4Tkj1z5"
      },
      "source": [
        "音声データのメタデータ取得\n",
        "----------------------\n",
        "\n",
        "``torchaudio.info``関数を用いて音声データのメタデータを取得できます。\n",
        "\n",
        "この関数にはファイルパス（のオブジェクト）かファイル（のオブジェクト）を渡すことができます。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oswri4hkj1z5"
      },
      "source": [
        "metadata = torchaudio.info(SAMPLE_WAV_PATH)\n",
        "print_metadata(metadata, src=SAMPLE_WAV_PATH)\n",
        "\n",
        "# 日本語訳注\n",
        "# 上記で使用しているSAMPLE_WAV_PATHは、\n",
        "# セル「データとユーティリティ関数の準備\n",
        "# このセルの中身を確認する必要はありません。\n",
        "# 本セルを一度実行して、次のセルへと進んでください」を\n",
        "# 実行した際に作られる変数です"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvUobwnfj1z6"
      },
      "source": [
        "各出力の意味は以下の通りです。\n",
        "\n",
        "-  ``sample_rate`` 音声データのサンプリングレート\n",
        "-  ``num_channels`` チャネル数\n",
        "-  ``num_frames`` チャネル毎のフレーム数\n",
        "-  ``bits_per_sample`` ビット深度\n",
        "-  ``encoding`` エンコードのフォーマット\n",
        "\n",
        "``encoding`` は以下のいずれかの値をとります。\n",
        "\n",
        "-  ``\"PCM_S\"``: 符号付き整数型リニア PCM\n",
        "-  ``\"PCM_U\"``: 符号なし整数型リニア PCM\n",
        "-  ``\"PCM_F\"``: 浮動小数点数型リニア PCM\n",
        "-  ``\"FLAC\"``: Flac, [Free Lossless Audio\n",
        "   Codec](https://xiph.org/flac/)\n",
        "-  ``\"ULAW\"``: Mu-law,\n",
        "   [\\[wikipedia\\]](https://en.wikipedia.org/wiki/%CE%9C-law_algorithm)\n",
        "-  ``\"ALAW\"``: A-law\n",
        "   [\\[wikipedia\\]](https://en.wikipedia.org/wiki/A-law_algorithm)\n",
        "-  ``\"MP3\"`` : MP3, MPEG-1 Audio Layer III\n",
        "-  ``\"VORBIS\"``: OGG Vorbis [\\[xiph.org\\]](https://xiph.org/vorbis/)\n",
        "-  ``\"AMR_NB\"``: Adaptive Multi-Rate\n",
        "   [\\[wikipedia\\]](https://en.wikipedia.org/wiki/Adaptive_Multi-Rate_audio_codec)\n",
        "-  ``\"AMR_WB\"``: Adaptive Multi-Rate Wideband\n",
        "   [\\[wikipedia\\]](https://en.wikipedia.org/wiki/Adaptive_Multi-Rate_Wideband)\n",
        "-  ``\"OPUS\"``: Opus [\\[opus-codec.org\\]](https://opus-codec.org/)\n",
        "-  ``\"GSM\"``: GSM-FR\n",
        "   [\\[wikipedia\\]](https://en.wikipedia.org/wiki/Full_Rate)\n",
        "-  ``\"UNKNOWN\"`` 上記のいずれにも該当しないフォーマット\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB25d0Wfj1z6"
      },
      "source": [
        "**注意**\n",
        "\n",
        "- データが圧縮されていたり可変ビットレートであったりする場合 (mp3 など) は ``bits_per_sample`` は ``0`` となることがあります。\n",
        "-  GSM-FRフォーマットでは ``num_frames`` が ``0`` になることがあります。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgpyJodPj1z6"
      },
      "source": [
        "metadata = torchaudio.info(SAMPLE_MP3_PATH)\n",
        "print_metadata(metadata, src=SAMPLE_MP3_PATH)\n",
        "\n",
        "metadata = torchaudio.info(SAMPLE_GSM_PATH)\n",
        "print_metadata(metadata, src=SAMPLE_GSM_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnGr6t6Yj1z7"
      },
      "source": [
        "### ファイル・オブジェクトの情報取得\n",
        "\n",
        "``info``関数は ファイルオブジェクト（file-like オブジェクト）に対しても使用可能です。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UTpNYc-j1z7"
      },
      "source": [
        "with requests.get(SAMPLE_WAV_URL, stream=True) as response:\n",
        "  metadata = torchaudio.info(response.raw)\n",
        "print_metadata(metadata, src=SAMPLE_WAV_URL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zelBNkiqj1z7"
      },
      "source": [
        "**注意** \n",
        "\n",
        "file-like オブジェクトを渡すと ``info`` 関数はデータ全体ではなく、データの最初の領域だけを読み取ります。\n",
        "\n",
        "従って、音声データのフォーマットによっては、フォーマット自体の情報を含む、メタデータを正しく読み取れないことがあります。以下はその例です。\n",
        "\n",
        "-  ``format`` 引数を使って音声データのフォーマットを明示する。\n",
        "-  返されるメタデータでは ``num_frames = 0`` となっている。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3jU0B9Zj1z7"
      },
      "source": [
        "with requests.get(SAMPLE_MP3_URL, stream=True) as response:\n",
        "  metadata = torchaudio.info(response.raw, format=\"mp3\")\n",
        "\n",
        "  print(f\"Fetched {response.raw.tell()} bytes.\")\n",
        "print_metadata(metadata, src=SAMPLE_MP3_URL)\n",
        "\n",
        "\n",
        "# 日本語訳注\n",
        "# SAMPLE_MP3_URLをファイルパスのオブジェクトとしてinfoに渡した3つほど上のセルでは\n",
        "# きちんと情報が取得できていますが、パスからファイルを取ってきてファイルのオブジェクトで\n",
        "# 渡した本セルでは、num_framesなどが0になりきちんと、読み取れていません。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suZE4PpUj1z7"
      },
      "source": [
        "音声データを読み込みテンソルにする\n",
        "------------------------------\n",
        "\n",
        "音声データの読み込みには ``torchaudio.load`` 関数が使えます。\n",
        "\n",
        "この関数は path-like オブジェクトと file-like オブジェクトを受け取ることができます。\n",
        "\n",
        "<br>\n",
        "\n",
        "戻り値は波形データ (``Tensor``) のタプルとサンプルレート (``int``) です。\n",
        "\n",
        "デフォルトでは、返されるテンソルオブジェクトの値は ``dtype=torch.float32`` であり、範囲は ``[-1.0, 1.0]`` に正規化されます。\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "サポートされているフォーマットの一覧は [torchaudio のドキュメント](https://pytorch.org/audio)を参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diE4tO6zj1z7"
      },
      "source": [
        "waveform, sample_rate = torchaudio.load(SAMPLE_WAV_SPEECH_PATH)\n",
        "\n",
        "print_stats(waveform, sample_rate=sample_rate)\n",
        "plot_waveform(waveform, sample_rate)\n",
        "plot_specgram(waveform, sample_rate)\n",
        "play_audio(waveform, sample_rate)\n",
        "\n",
        "# 日本語訳注\n",
        "# 上記で使用しているplot_waveformなどの描画関数、\n",
        "# play_audioという音声再生関数は\n",
        "#\n",
        "# セル「データとユーティリティ関数の準備\n",
        "# このセルの中身を確認する必要はありません。\n",
        "# 本セルを一度実行して、次のセルへと進んでください」\n",
        "#\n",
        "#を実行した際に定義されています"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL4qvX0eZ7Yj"
      },
      "source": [
        "**日本語訳注**\n",
        "\n",
        "この音声データは今後も使うのですが、おそらく\n",
        "\n",
        "I had the curiosity beside me this moment.\n",
        "\n",
        "もしくは\n",
        "\n",
        "I had that curiosity beside me at this moment.（Amazon Transcribeで文字にした場合）\n",
        "\n",
        "と言っており、\n",
        "\n",
        "<br>\n",
        "\n",
        "「私はこのときから、好奇心を抱くようになりました。」\n",
        "\n",
        "もしくは\n",
        "\n",
        "「私はこの瞬間、好奇心を持っていました。」という意味でしょうか。。\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z5ibKFfj1z8"
      },
      "source": [
        "### file-like オブジェクトの読み込み\n",
        "\n",
        "``torchaudio``の入出力関数は file-like オブジェクトをサポートしています。\n",
        "\n",
        "そのためローカル以外の場所から、音声データ取得とデコードを同時に行うことが可能です。\n",
        "\n",
        "次のセルで例を示します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V43E8Acaj1z8"
      },
      "source": [
        "# HTTPリクエストとして音声データを読み込む\n",
        "with requests.get(SAMPLE_WAV_SPEECH_URL, stream=True) as response:\n",
        "  waveform, sample_rate = torchaudio.load(response.raw)\n",
        "plot_specgram(waveform, sample_rate, title=\"HTTP datasource\")\n",
        "\n",
        "# tarファイルから音声データを読み込む\n",
        "with tarfile.open(SAMPLE_TAR_PATH, mode='r') as tarfile_:\n",
        "  fileobj = tarfile_.extractfile(SAMPLE_TAR_ITEM)\n",
        "  waveform, sample_rate = torchaudio.load(fileobj)\n",
        "plot_specgram(waveform, sample_rate, title=\"TAR file\")\n",
        "\n",
        "# S3から音声データを読み込む\n",
        "client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "response = client.get_object(Bucket=S3_BUCKET, Key=S3_KEY)\n",
        "waveform, sample_rate = torchaudio.load(response['Body'])\n",
        "plot_specgram(waveform, sample_rate, title=\"From S3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BdjrynEj1z8"
      },
      "source": [
        "### スライシングのTips\n",
        "\n",
        "``num_frames`` と ``frame_offset`` を引数に与えるとデコード時にスライスされたテンソルが返されます。\n",
        "\n",
        "通常のテンソルのスライシング (``waveform[:, frame_offset:frame_offset+num_frames]``) でも同じ結果は得られますが、``num_frames`` と ``frame_offset`` 引数を使う方が効率的です。\n",
        "\n",
        "<br>\n",
        "\n",
        "なぜならデコード時に、 ``torchaudio.load`` 関数が指定されたフレームが完了した段階でデータの取得とデコードを終えるからです。\n",
        "\n",
        "必要なデータが取得された時点でデータ転送が停止するため、ネットワークを通じて音声データを転送する場合にこの仕様は有用です。\n",
        "\n",
        "\n",
        "次の例で説明します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM24av6-j1z8"
      },
      "source": [
        "# 異なる2種類のデコード手法を説明します。\n",
        "# 1つ目はデータ全体を取得し、それからデコードする方法です。\n",
        "# 2つ目は必要なデコードが完了した時点でデータの取得を終了する方法です。\n",
        "# 結果として得られる波形は同一のものとなります。\n",
        "# ですが、取得したデータ量は2つ目の方法が少なく済みます。\n",
        "\n",
        "frame_offset, num_frames = 16000, 16000  # 1秒から2秒のデータを取得しデコードする\n",
        "\n",
        "# [1]\n",
        "print(\"データ全体を取得...\")\n",
        "with requests.get(SAMPLE_WAV_SPEECH_URL, stream=True) as response:\n",
        "  waveform1, sample_rate1 = torchaudio.load(response.raw)\n",
        "  waveform1 = waveform1[:, frame_offset:frame_offset+num_frames]\n",
        "  print(f\" - {response.raw.tell()} バイトのデータを取得しました。\")\n",
        "\n",
        "# [2]\n",
        "print(\"要求されたフレームが利用可能になるまでデータを取得...\")\n",
        "with requests.get(SAMPLE_WAV_SPEECH_URL, stream=True) as response:\n",
        "  waveform2, sample_rate2 = torchaudio.load(\n",
        "      response.raw, frame_offset=frame_offset, num_frames=num_frames)\n",
        "  print(f\" - {response.raw.tell()} バイトのデータを取得しました。\")\n",
        "\n",
        "print(\"得られた波形の確認 ... \", end=\"\")\n",
        "assert (waveform1 == waveform2).all()\n",
        "print(\"同一の波形です\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiU1z2eij1z8"
      },
      "source": [
        "音声データをファイルへ保存する\n",
        "--------------------\n",
        "\n",
        "``torchaudio.save`` 関数を使い、一般的なアプリケーションで取り扱える\n",
        "フォーマットで音声データを保存することができます。\n",
        "\n",
        "この関数はpath-likeオブジェクトとfile-likeオブジェクトを受け取ることができます。\n",
        "\n",
        "<br>\n",
        "\n",
        "file-likeオブジェクトを渡す場合は、フォーマットを明示するために ``format`` 引数が必要です。\n",
        "\n",
        "path-like オブジェクトの場合は拡張子からフォーマットが決定されます。拡張子なしで保存する場合は``format`` 引数が必要です。\n",
        "\n",
        "<br>\n",
        "\n",
        "WAVフォーマット保存時、``float32``型のテンソルに対するデフォルトのエンコードは32ビット浮動小数点数CPMです。``encoding``と``bits_per_sample``引数によって変更可能です。\n",
        "\n",
        "例えば16ビット符号付き整数PCMで保存をするには以下のセルのように実行します。\n",
        "\n",
        "<br>\n",
        "\n",
        "**注意** \n",
        "\n",
        "少ないビット深度でデータを保存するとファイルサイズは削減されますが精度が低下します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF51ukq6j1z9"
      },
      "source": [
        "waveform, sample_rate = get_sample()\n",
        "print_stats(waveform, sample_rate=sample_rate)\n",
        "\n",
        "# エンコードを指定せずに保存する\n",
        "# データに適合したエンコードが自動的に選択される\n",
        "path = \"save_example_default.wav\"\n",
        "torchaudio.save(path, waveform, sample_rate)\n",
        "inspect_file(path)\n",
        "\n",
        "# 16ビット符号付き整数リニアPCMとして保存する\n",
        "# ファイルサイズは半分になりますが、精度が低下する\n",
        "path = \"save_example_PCM_S16.wav\"\n",
        "torchaudio.save(\n",
        "    path, waveform, sample_rate,\n",
        "    encoding=\"PCM_S\", bits_per_sample=16)\n",
        "inspect_file(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdgmU22sj1z9"
      },
      "source": [
        "``torchaudio.save`` は他のフォーマットにも対応しています。いくつか例を挙げます。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q6PD7Jvj1z9"
      },
      "source": [
        "waveform, sample_rate = get_sample(resample=8000)\n",
        "\n",
        "formats = [\n",
        "  \"mp3\",\n",
        "  \"flac\",\n",
        "  \"vorbis\",\n",
        "  \"sph\",\n",
        "  \"amb\",\n",
        "  \"amr-nb\",\n",
        "  \"gsm\",\n",
        "]\n",
        "\n",
        "for format in formats:\n",
        "  path = f\"save_example.{format}\"\n",
        "  torchaudio.save(path, waveform, sample_rate, format=format)\n",
        "  inspect_file(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAp607p8j1z9"
      },
      "source": [
        "### file-likeオブジェクトへ保存する\n",
        "\n",
        "音声データをfile-likeオブジェクトに保存することができます。\n",
        "\n",
        "\n",
        "file-likeオブジェクトに保存する場合、``format``引数が必須となります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDsFXFgpj1z9"
      },
      "source": [
        "waveform, sample_rate = get_sample()\n",
        "\n",
        "# バイトバッファへ保存する\n",
        "buffer_ = io.BytesIO()\n",
        "torchaudio.save(buffer_, waveform, sample_rate, format=\"wav\")\n",
        "\n",
        "buffer_.seek(0)\n",
        "print(buffer_.read(16))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tWzQyEFj1z9"
      },
      "source": [
        "7.3 データオーギュメンテーション（DATA AUGMENTATION）\n",
        "=================\n",
        "\n",
        "``torchaudio`` には様々なデータオーギュメンテーション手法が用意されています。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPYu_hRij1z-"
      },
      "source": [
        "エフェクトとフィルタを適用する\n",
        "------------------------------\n",
        "\n",
        "``torchaudio.sox_effects``モジュールを使うと音声データのテンソルオブジェクトやファイルオブジェクトに対し、``sox``コマンドのように直接フィルタを適用できます。\n",
        "\n",
        "<br>\n",
        "\n",
        "フィルタの適用には2つの関数があります。\n",
        "\n",
        "-  ``torchaudio.sox_effects.apply_effects_tensor`` はテンソルにエフェクトを適用します\n",
        "-  ``torchaudio.sox_effects.apply_effects_file`` はその他の音声データソースにエフェクトを適用します\n",
        "\n",
        "<br>\n",
        "\n",
        "どちらの関数もエフェクトを``List[List[str]]``の形式で指定できます。\n",
        "\n",
        "これは``sox``コマンドの動作にほぼ一致していますが、1つ注意が必要なのは\n",
        "``sox``コマンドは自動的にいくつかのエフェクトが追加されるのに対して、\n",
        "torchaudioの実装はそうなっていない点です。\n",
        "\n",
        "利用可能なエフェクトの一覧は[soxのドキュメント](http://sox.sourceforge.net/sox.html)を御覧ください。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGlcpWPfUbC9"
      },
      "source": [
        "**Tip** \n",
        "\n",
        "音声データをリアルタイムに読み込んでリサンプルする必要がある場合、\n",
        "``torchaudio.sox_effects.apply_effects_file``で``\"rate\"``を使用してください。\n",
        "\n",
        "**注意** \n",
        "\n",
        "``apply_effects_file`` 関数はfile-likeオブジェクトもpath-likeオブジェクトも受け取ることができます。\n",
        "\n",
        "``torchaudio.load``と同じく、ファイルの拡張子かヘッダーから音声フォーマットが読み取れない場合、``format``引数を使い関数に音声データのフォーマットを知らせることができます。\n",
        "\n",
        "**注意** \n",
        "\n",
        "この操作は微分不可能です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlExO8pJj1z-"
      },
      "source": [
        "# データを読み込む\n",
        "waveform1, sample_rate1 = get_sample(resample=16000)\n",
        "\n",
        "# エフェクトを定義する\n",
        "effects = [\n",
        "  [\"lowpass\", \"-1\", \"300\"], # ローパスフィルター(単極)の適用\n",
        "  [\"speed\", \"0.8\"],  # 速度を下げる\n",
        "                     # サンプリングレートだけが変更されるので、 \n",
        "                     # この後に元のサンプリングレートの値で\n",
        "                     # `rate`エフェクトを適用する必要がある\n",
        "  [\"rate\", f\"{sample_rate1}\"],\n",
        "  [\"reverb\", \"-w\"],  # 反響のエフェクトを加えるとドラマチックな雰囲気になる\n",
        "]\n",
        "\n",
        "# エフェクトを適用する\n",
        "waveform2, sample_rate2 = torchaudio.sox_effects.apply_effects_tensor(\n",
        "    waveform1, sample_rate1, effects)\n",
        "\n",
        "plot_waveform(waveform1, sample_rate1, title=\"Original\", xlim=(-.1, 3.2))\n",
        "plot_waveform(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(-.1, 3.2))\n",
        "print_stats(waveform1, sample_rate=sample_rate1, src=\"Original\")\n",
        "print_stats(waveform2, sample_rate=sample_rate2, src=\"Effects Applied\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XwIsRVaj1z-"
      },
      "source": [
        "エフェクト適用後にフレーム数とチャンネル数が元のデータから変わっていることに注意してください。\n",
        "\n",
        "実際に音声を聞いてみましょう。\n",
        "\n",
        "データオーギュメンテーションの操作により音声が変化しており、後者の方がよりドラマチックに感じませんか。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMxeENw1j1z-"
      },
      "source": [
        "plot_specgram(waveform1, sample_rate1, title=\"Original\", xlim=(0, 3.04))\n",
        "play_audio(waveform1, sample_rate1)\n",
        "plot_specgram(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(0, 3.04))\n",
        "play_audio(waveform2, sample_rate2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6utRmhtj1z-"
      },
      "source": [
        "室内での反響をシミュレーションする\n",
        "----------------------------\n",
        "\n",
        "[Convolution reverb](https://en.wikipedia.org/wiki/Convolution_reverb) は異なる環境で音が鳴っているかのような、クリーンな音声データを生成する際に使われる技術です。\n",
        "\n",
        "<br>\n",
        "\n",
        "Room Impluse Response (RIR, 室内インパルス応答)により、会議室で発せられたようなクリーンな音声データを生成できます。\n",
        "\n",
        "<br>\n",
        "\n",
        "この処理を行うためにはRIRデータが必要です。\n",
        "\n",
        "次のセルではVOiCESデータセットの音声を使用していますが、ご自身で録音した音声データを使うこともできます。\n",
        "\n",
        "マイクを有効にし、ご自身で手を叩いて拍手の音を収録してみてください。\n",
        "\n",
        "<br>\n",
        "\n",
        "日本語訳注：\n",
        "\n",
        "ご自身が扱いたい、特定の部屋や環境での反響をシミュレーションする際には、その反響のデータとして、一度拍手をした音声データ（インパルス応答のデータ）を用意します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QywZity8j1z_"
      },
      "source": [
        "sample_rate = 8000\n",
        "\n",
        "rir_raw, _ = get_rir_sample(resample=sample_rate)\n",
        "\n",
        "plot_waveform(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\", ylim=None)\n",
        "plot_specgram(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\n",
        "play_audio(rir_raw, sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joAZUgsmj1z_"
      },
      "source": [
        "最初にRIRを整えます。主要インパルスを抽出し、振幅を正規化し、時間軸を反転します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le6ahrzHj1z_"
      },
      "source": [
        "rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
        "rir = rir / torch.norm(rir, p=2)\n",
        "rir = torch.flip(rir, [1])\n",
        "\n",
        "print_stats(rir)\n",
        "plot_waveform(rir, sample_rate, title=\"Room Impulse Response\", ylim=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hxKnvfqj1z_"
      },
      "source": [
        "次にスピーチの信号とRIRフィルターの畳み込みを行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWGOm7T6j1z_"
      },
      "source": [
        "speech, _ = get_speech_sample(resample=sample_rate)\n",
        "\n",
        "speech_ = torch.nn.functional.pad(speech, (rir.shape[1]-1, 0))\n",
        "augmented = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n",
        "\n",
        "plot_waveform(speech, sample_rate, title=\"Original\", ylim=None)\n",
        "plot_waveform(augmented, sample_rate, title=\"RIR Applied\", ylim=None)\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"Original\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "plot_specgram(augmented, sample_rate, title=\"RIR Applied\")\n",
        "play_audio(augmented, sample_rate)\n",
        "\n",
        "# 日本語訳注\n",
        "# 再生ファイルが以下に2つ作成されます。\n",
        "# 1つ目はOriginalのクリーンなファイルです。\n",
        "# 2つ目は拍手で取得した室内反響をOriginalなデータに畳み込み、室内で収録したような音声にしたファイルです。\n",
        "# 2つのデータを再生し、違いを感じてみてください。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIrPP2eAj1z_"
      },
      "source": [
        "バックグラウンドノイズの付加\n",
        "-----------------------\n",
        "\n",
        "音声データにバックグラウンドノイズを付加するには、単に音声データのテンソルと\n",
        "ノイズテンソルを足し合わせます。\n",
        "\n",
        "<be>\n",
        "\n",
        "ノイズの強度を調整する一般的な手法には[Signal-to-Noise Ratio (SNR、信号雑音比)](https://ja.wikipedia.org/wiki/SN%E6%AF%94#:~:text=SN%E6%AF%94%EF%BC%88%E3%82%A8%E3%82%B9%E3%82%A8%E3%83%8C%E3%81%B2%EF%BC%89%E3%81%AF,%E3%80%81S%2FN%E3%81%A8%E3%82%82%E7%95%A5%E3%81%99%E3%80%82)があります。\n",
        "\n",
        "\\begin{align}\\mathrm{SNR} = \\frac{P_\\mathrm{signal}}{P_\\mathrm{noise}}\\end{align}\n",
        "\n",
        "\\begin{align}{\\mathrm  {SNR_{{dB}}}}=10\\log _{{10}}\\left({\\mathrm  {SNR}}\\right)\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUMG6vCHj10A"
      },
      "source": [
        "sample_rate = 8000\n",
        "speech, _ = get_speech_sample(resample=sample_rate)\n",
        "noise, _ = get_noise_sample(resample=sample_rate)\n",
        "noise = noise[:, :speech.shape[1]]\n",
        "\n",
        "plot_waveform(noise, sample_rate, title=\"Background noise\")\n",
        "plot_specgram(noise, sample_rate, title=\"Background noise\")\n",
        "play_audio(noise, sample_rate)\n",
        "\n",
        "speech_power = speech.norm(p=2)\n",
        "noise_power = noise.norm(p=2)\n",
        "\n",
        "for snr_db in [20, 10, 3]:\n",
        "  snr = math.exp(snr_db / 10)\n",
        "  scale = snr * noise_power / speech_power\n",
        "  noisy_speech = (scale * speech + noise) / 2\n",
        "\n",
        "  plot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "  plot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "  play_audio(noisy_speech, sample_rate)\n",
        "\n",
        "\n",
        "# 日本語訳注\n",
        "# 出力結果に4つの音声再生が用意されます。\n",
        "# 1つ目はバックグラウンドノイズ音声の再生です。\n",
        "# snr_dbが20,10,3のため、\n",
        "# 2つ目から4つ目にかけて、音声再生の上から順にノイズが強い音声になっていることを確認してみてください。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIDJflbbj10A"
      },
      "source": [
        "テンソルオブジェクトにコーデックを適用する\n",
        "-------------------------------\n",
        "\n",
        "``torchaudio.functional.apply_codec``を使ってテンソルオブジェクトにコーデックを適用することができます。\n",
        "\n",
        "**注意** この処理は微分不可能です。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fawlqlkj10A"
      },
      "source": [
        "waveform, sample_rate = get_speech_sample(resample=8000)\n",
        "\n",
        "plot_specgram(waveform, sample_rate, title=\"Original\")\n",
        "play_audio(waveform, sample_rate)\n",
        "\n",
        "configs = [\n",
        "    ({\"format\": \"wav\", \"encoding\": 'ULAW', \"bits_per_sample\": 8}, \"8 bit mu-law\"),\n",
        "    ({\"format\": \"gsm\"}, \"GSM-FR\"),\n",
        "    ({\"format\": \"mp3\", \"compression\": -9}, \"MP3\"),\n",
        "    ({\"format\": \"vorbis\", \"compression\": -1}, \"Vorbis\"),\n",
        "]\n",
        "for param, title in configs:\n",
        "  augmented = F.apply_codec(waveform, sample_rate, **param)\n",
        "  plot_specgram(augmented, sample_rate, title=title)\n",
        "  play_audio(augmented, sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ZiXB5Zj10A"
      },
      "source": [
        "電話音声のシミュレーション\n",
        "---------------------------\n",
        "\n",
        "これまでの操作を組み合わせて、反響する部屋で、バックグラウンドで人々が話している中、電話を使って話している人の音声をシミュレーションできます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvhqifD8j10A"
      },
      "source": [
        "sample_rate = 16000\n",
        "speech, _ = get_speech_sample(resample=sample_rate)\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"Original\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "# RIRを適用する\n",
        "rir, _ = get_rir_sample(resample=sample_rate, processed=True)\n",
        "speech_ = torch.nn.functional.pad(speech, (rir.shape[1]-1, 0))\n",
        "speech = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"RIR Applied\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "# バックグラウンドノイズを付加する\n",
        "# ノイズは実環境において録音されたものなので、ノイズはその環境における音声特性を\n",
        "# 既に持っていると考えます。従って、ノイズはRIRを適用した後に付加します。\n",
        "noise, _ = get_noise_sample(resample=sample_rate)\n",
        "noise = noise[:, :speech.shape[1]]\n",
        "\n",
        "snr_db = 8\n",
        "scale = math.exp(snr_db / 10) * noise.norm(p=2) / speech.norm(p=2)\n",
        "speech = (scale * speech + noise) / 2\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"BG noise added\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "# フィルタの適用し、サンプリングレートを変更する\n",
        "speech, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n",
        "  speech,\n",
        "  sample_rate,\n",
        "  effects=[\n",
        "      [\"lowpass\", \"4000\"],\n",
        "      [\"compand\", \"0.02,0.05\", \"-60,-60,-30,-10,-20,-8,-5,-8,-2,-8\", \"-8\", \"-7\", \"0.05\"],\n",
        "      [\"rate\", \"8000\"],\n",
        "  ],\n",
        ")\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"Filtered\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "# 電話用コーデックを適用する\n",
        "speech = F.apply_codec(speech, sample_rate, format=\"gsm\")\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"GSM Codec Applied\")\n",
        "play_audio(speech, sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeMYSs9_j10A"
      },
      "source": [
        "7.4 特徴量抽出（FEATURE EXTRACTIONS）\n",
        "===================\n",
        "\n",
        "``torchaudio``には音声データの領域で広く使われている特徴量抽出手法が\n",
        "用意されています。\n",
        "\n",
        "それらは``torchaudio.functional``と``torchaudio.transforms``\n",
        "で利用可能です。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwO3KxLIcNoY"
      },
      "source": [
        "\n",
        "``functional``モジュールは単一の関数として機能を実装しています。\n",
        "これらの関数はステートレスです。\n",
        "\n",
        "<br>\n",
        "\n",
        "``transforms``モジュールは、``functional``と``torch.nn.Module``の実装を用いて、オブジェクト指向で機能を実装しています。\n",
        "\n",
        "すべての``transforms``は``torch.nn.Module``のサブクラスなので、TorchScriptによって繋げることが可能です。\n",
        "\n",
        "\n",
        "利用可能な機能の一覧は[ドキュメント](https://pytorch.org/audio/stable/transforms.html)を参照してください。\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "本チュートリアルでは、時間領域と周波数領域間の変換 (``Spectrogram``, ``GriffinLim``,``MelSpectrogram``)とSpecAugomentと呼ばれるオーギュメンテーション手法 について説明します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLGVRUP1j10A"
      },
      "source": [
        "スペクトログラム\n",
        "-----------\n",
        "\n",
        "``Spectrogram``変換により、音声信号の周波数表現が得られます。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N01m0PJqj10B"
      },
      "source": [
        "waveform, sample_rate = get_speech_sample()\n",
        "\n",
        "n_fft = 1024\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "\n",
        "# 変換の定義\n",
        "spectrogram = T.Spectrogram(\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        ")\n",
        "# 変換の実行\n",
        "spec = spectrogram(waveform)\n",
        "\n",
        "print_stats(spec)\n",
        "plot_spectrogram(spec[0], title='torchaudio')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UAcy0GLj10B"
      },
      "source": [
        "GriffinLim\n",
        "----------\n",
        "\n",
        "``GriffinLim``により、スペクトログラムから波形データを復元できます。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE11MAogj10B"
      },
      "source": [
        "torch.random.manual_seed(0)\n",
        "waveform, sample_rate = get_speech_sample()\n",
        "plot_waveform(waveform, sample_rate, title=\"Original\")\n",
        "play_audio(waveform, sample_rate)\n",
        "\n",
        "n_fft = 1024\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "\n",
        "spec = T.Spectrogram(\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        ")(waveform)\n",
        "\n",
        "griffin_lim = T.GriffinLim(\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        ")\n",
        "waveform = griffin_lim(spec)\n",
        "\n",
        "plot_waveform(waveform, sample_rate, title=\"Reconstructed\")\n",
        "play_audio(waveform, sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7vEQDnXj10B"
      },
      "source": [
        "メルフィルタバンク\n",
        "---------------\n",
        "\n",
        "``torchaudio.functional.create_fb_matrix``は周波数ビンをメルスケールビンに変換するためのフィルタバンクを生成します。\n",
        "\n",
        "この関数は入力の音声データ、特徴量を必要としないため``torchaudio.transforms``には対応した関数がありません。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y9tuEDnj10B"
      },
      "source": [
        "n_fft = 256\n",
        "n_mels = 64\n",
        "sample_rate = 6000\n",
        "\n",
        "mel_filters = F.create_fb_matrix(\n",
        "    int(n_fft // 2 + 1),\n",
        "    n_mels=n_mels,\n",
        "    f_min=0.,\n",
        "    f_max=sample_rate/2.,\n",
        "    sample_rate=sample_rate,\n",
        "    norm='slaney'\n",
        ")\n",
        "plot_mel_fbank(mel_filters, \"Mel Filter Bank - torchaudio\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3GtCAaoj10B"
      },
      "source": [
        "### librosaとの比較\n",
        "\n",
        "比較として、``librosa``で同様のメルフィルタバンクを生成します。\n",
        "\n",
        "**注意** \n",
        "\n",
        "現状では、``htk=True``時のみ結果は一致します。\n",
        "\n",
        "``htk=False``時の結果一致は``torchaudio``はサポートしていません。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hyUX3tCj10B"
      },
      "source": [
        "mel_filters_librosa = librosa.filters.mel(\n",
        "    sample_rate,\n",
        "    n_fft,\n",
        "    n_mels=n_mels,\n",
        "    fmin=0.,\n",
        "    fmax=sample_rate/2.,\n",
        "    norm='slaney',\n",
        "    htk=True,\n",
        ").T\n",
        "\n",
        "plot_mel_fbank(mel_filters_librosa, \"Mel Filter Bank - librosa\")\n",
        "\n",
        "mse = torch.square(mel_filters - mel_filters_librosa).mean().item()\n",
        "print('Mean Square Difference: ', mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4srAilg5dMqR"
      },
      "source": [
        "**日本語訳注**\n",
        "\n",
        "librosaは本チュートリアルの冒頭でpip installしていますが、\n",
        "\n",
        "Pythonの音楽分析用モジュール 「LibROSA」の意味です。\n",
        "\n",
        "https://librosa.org/doc/latest/index.html\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmmDYa_1j10C"
      },
      "source": [
        "メルスペクトログラム\n",
        "--------------\n",
        "\n",
        "メルスケールスペクトログラムはスペクトログラムとメルスケール変換の組み合わせです。\n",
        "\n",
        "``torchaudio``には``Spectrogram``と``MelScale``を組み合わせた``MelSpectrogram``変換が実装されています。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5hJ2EiPj10C"
      },
      "source": [
        "waveform, sample_rate = get_speech_sample()\n",
        "\n",
        "n_fft = 1024\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_mels = 128\n",
        "\n",
        "mel_spectrogram = T.MelSpectrogram(\n",
        "    sample_rate=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        "    norm='slaney',\n",
        "    onesided=True,\n",
        "    n_mels=n_mels,\n",
        ")\n",
        "\n",
        "melspec = mel_spectrogram(waveform)\n",
        "plot_spectrogram(\n",
        "    melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel='mel freq')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0Uzckzdj10C"
      },
      "source": [
        "### librosaとの比較\n",
        "\n",
        "比較として、``librosa``で同様のメルスケールスペクトログラムを生成します。\n",
        "\n",
        "**注意** \n",
        "\n",
        "現状では、``htk=True``時のみ結果は一致します。\n",
        "\n",
        "``htk=False``時の結果一致は``torchaudio``はサポートしていません。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pInkvSu1j10C"
      },
      "source": [
        "melspec_librosa = librosa.feature.melspectrogram(\n",
        "    waveform.numpy()[0],\n",
        "    sr=sample_rate,\n",
        "    n_fft=n_fft,\n",
        "    hop_length=hop_length,\n",
        "    win_length=win_length,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        "    n_mels=n_mels,\n",
        "    norm='slaney',\n",
        "    htk=True,\n",
        ")\n",
        "plot_spectrogram(\n",
        "    melspec_librosa, title=\"MelSpectrogram - librosa\", ylabel='mel freq')\n",
        "\n",
        "mse = torch.square(melspec - melspec_librosa).mean().item()\n",
        "print('Mean Square Difference: ', mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7420MLACj10C"
      },
      "source": [
        "MFCC (メル周波数ケプストラム係数)\n",
        "----\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td-dckqwj10C"
      },
      "source": [
        "waveform, sample_rate = get_speech_sample()\n",
        "\n",
        "n_fft = 2048\n",
        "win_length = None\n",
        "hop_length = 512\n",
        "n_mels = 256\n",
        "n_mfcc = 256\n",
        "\n",
        "mfcc_transform = T.MFCC(\n",
        "    sample_rate=sample_rate,\n",
        "    n_mfcc=n_mfcc, melkwargs={'n_fft': n_fft, 'n_mels': n_mels, 'hop_length': hop_length})\n",
        "\n",
        "mfcc = mfcc_transform(waveform)\n",
        "\n",
        "plot_spectrogram(mfcc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5kASrwwj10C"
      },
      "source": [
        "### librosaとの比較"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRRCFi4lj10D"
      },
      "source": [
        "melspec = librosa.feature.melspectrogram(\n",
        "  y=waveform.numpy()[0], sr=sample_rate, n_fft=n_fft,\n",
        "  win_length=win_length, hop_length=hop_length,\n",
        "  n_mels=n_mels, htk=True, norm=None)\n",
        "\n",
        "mfcc_librosa = librosa.feature.mfcc(\n",
        "  S=librosa.core.spectrum.power_to_db(melspec),\n",
        "  n_mfcc=n_mfcc, dct_type=2, norm='ortho')\n",
        "\n",
        "plot_spectrogram(mfcc_librosa)\n",
        "\n",
        "mse = torch.square(mfcc - mfcc_librosa).mean().item()\n",
        "print('平均二乗誤差: ', mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYJpsiO-j10D"
      },
      "source": [
        "ピッチ\n",
        "-----\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51BB_RKGj10D"
      },
      "source": [
        "waveform, sample_rate = get_speech_sample()\n",
        "\n",
        "pitch = F.detect_pitch_frequency(waveform, sample_rate)\n",
        "plot_pitch(waveform, sample_rate, pitch)\n",
        "play_audio(waveform, sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLnCytnXj10D"
      },
      "source": [
        "カルディピッチ (beta)\n",
        "------------------\n",
        "\n",
        "カルディピッチ特徴量 [1] は自動音声認識アプリケーション用に調整されたピッチ検出手法です。\n",
        "\n",
        "この機能はベータ版であり、``functional``形式でのみ利用可能です。\n",
        "\n",
        "\n",
        "--- \n",
        "\n",
        "1. A pitch extraction algorithm tuned for automatic speech recognition\n",
        "\n",
        "   Ghahremani, B. BabaAli, D. Povey, K. Riedhammer, J. Trmal and S.\n",
        "   Khudanpur\n",
        "\n",
        "   2014 IEEE International Conference on Acoustics, Speech and Signal\n",
        "   Processing (ICASSP), Florence, 2014, pp. 2494-2498, doi:\n",
        "   10.1109/ICASSP.2014.6854049. [[abstract](https://ieeexplore.ieee.org/document/6854049)], [[paper](https://danielpovey.com/files/2014_icassp_pitch.pdf)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9JB5RURj10D"
      },
      "source": [
        "waveform, sample_rate = get_speech_sample(resample=16000)\n",
        "\n",
        "pitch_feature = F.compute_kaldi_pitch(waveform, sample_rate)\n",
        "pitch, nfcc = pitch_feature[..., 0], pitch_feature[..., 1]\n",
        "\n",
        "plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc)\n",
        "play_audio(waveform, sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnDLx-grj10D"
      },
      "source": [
        "7.5 特徴量オーギュメンテーション（FEATURE AUGMENTATION）\n",
        "====================\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_MdIF0-j10D"
      },
      "source": [
        "SpecAugment\n",
        "-----------\n",
        "\n",
        "[SpecAugment](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html)はスペクトログラムに対して適応可能な、よく用いられているオーギュメンテーション手法です。\n",
        "\n",
        "``torchaudio``では``TimeStrech``、``TimeMasking``、\n",
        "``FrequencyMasking``を用意しています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWPkF3Rnj10D"
      },
      "source": [
        "### 時間方向の伸縮\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL1IQ22nj10E"
      },
      "source": [
        "spec = get_spectrogram(power=None)\n",
        "strech = T.TimeStretch()\n",
        "\n",
        "rate = 1.2\n",
        "spec_ = strech(spec, rate)\n",
        "plot_spectrogram(spec_[0].abs(), title=f\"Stretched x{rate}\", aspect='equal', xmax=304)\n",
        "\n",
        "plot_spectrogram(spec[0].abs(), title=\"Original\", aspect='equal', xmax=304)\n",
        "\n",
        "rate = 0.9\n",
        "spec_ = strech(spec, rate)\n",
        "plot_spectrogram(spec_[0].abs(), title=f\"Stretched x{rate}\", aspect='equal', xmax=304)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_JNg-4Lj10E"
      },
      "source": [
        "### 時間マスキング\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cpclWJkj10E"
      },
      "source": [
        "torch.random.manual_seed(4)\n",
        "\n",
        "spec = get_spectrogram()\n",
        "plot_spectrogram(spec[0], title=\"Original\")\n",
        "\n",
        "masking = T.TimeMasking(time_mask_param=80)\n",
        "spec = masking(spec)\n",
        "\n",
        "plot_spectrogram(spec[0], title=\"Masked along time axis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tfBJ__kj10E"
      },
      "source": [
        "### 周波数マスキング\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2j26FUQj10E"
      },
      "source": [
        "torch.random.manual_seed(4)\n",
        "\n",
        "spec = get_spectrogram()\n",
        "plot_spectrogram(spec[0], title=\"Original\")\n",
        "\n",
        "masking = T.FrequencyMasking(freq_mask_param=80)\n",
        "spec = masking(spec)\n",
        "\n",
        "plot_spectrogram(spec[0], title=\"Masked along frequency axis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy3S6TD_j10E"
      },
      "source": [
        "7.6 データセット（DATASETS）\n",
        "========\n",
        "\n",
        "``torchaudio``では、一般的によく使用されている音声データセットを簡単に使用することができます。\n",
        "\n",
        "利用可能なデータセットの一覧は[公式ドキュメント](https://pytorch.org/audio/stable/datasets.html)を参照してください。\n",
        "\n",
        "ここでは``YESNO``を例に使用方法を示します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilS3tDz1j10E"
      },
      "source": [
        "YESNO_DOWNLOAD_PROCESS.join()\n",
        "\n",
        "dataset = torchaudio.datasets.YESNO(YESNO_DATASET_PATH, download=True)\n",
        "\n",
        "for i in [1, 3, 5]:\n",
        "  waveform, sample_rate, label = dataset[i]\n",
        "  plot_specgram(waveform, sample_rate, title=f\"Sample {i}: {label}\")\n",
        "  play_audio(waveform, sample_rate)\n",
        "\n",
        "\n",
        "# 日本語訳注\n",
        "# YESNOデータセットでは、0か1のリスト（長さ8）に対応した音声を再生しています。\n",
        "# 以下データセットから3つのデータを取り出します。\n",
        "# データセットのラベルにはNo, Yesを示す長さ8のリストが入っています。\n",
        "# 音声を再生して確かめてください。\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB0JgkTof_MD"
      },
      "source": [
        "【日本語訳注】\n",
        "\n",
        "上記の音声を聞いていると、YesはCanのように聞こえるかと思います。\n",
        "\n",
        "これは、今回のデータセットOpenSLRの仕様が\n",
        "\n",
        ">「Yesno」\n",
        "Sixty recordings of one individual saying yes or no in Hebrew; each recording is eight words long.\n",
        "\n",
        "であり、Hebrew（ヘブライ語）での発音だからです。\n",
        "\n",
        "<br>\n",
        "\n",
        "Yesはヘブライ語で、כן　です。発音は以下のリンクの通りです（canに似ています）。\n",
        "\n",
        "（発音）\n",
        "https://ja.forvo.com/word/%D7%9B%D7%9F/\n",
        "\n",
        "<br> \n",
        "\n",
        "（OpenSLR）\n",
        "http://www.openslr.org/resources.php\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1awrNKxqf_ZF"
      },
      "source": [
        "以上。"
      ]
    }
  ]
}