{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/ML/blob/main/Trying_Langchain_RAG_with_Elyza_instruct_7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Elyza 7bを用いたRAGを試してみた。\n",
        "\n",
        "今回はLLMには[Elyza 7B Instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct)を用い、Langchainを使ったRAG (Retrieval Augmented Generation) を試してみました。\n",
        "\n",
        "RAGを用いることで質問に対して関連性の高い文章を抽出し、より適切な答えを導き出せることを期待します。\n"
      ],
      "metadata": {
        "id": "2NYeLEU3pgom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 必要なライブラリをインストール\n"
      ],
      "metadata": {
        "id": "MSgbKpUnpcTj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvYuJwdF6vps"
      },
      "outputs": [],
      "source": [
        "# To solve for an error encountered: `NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968`\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# 必要なライブラリをインストール\n",
        "!pip install transformers langchain accelerate bitsandbytes pypdf tiktoken sentence_transformers faiss-gpu trafilatura --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# テキストが見やすいようにwrapしておく\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "dk1El-ootXbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## コードを実行\n",
        "\n",
        "必要なライブラリをロードします。\n"
      ],
      "metadata": {
        "id": "z-MXidABqFwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "FYPTXrKI8bpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データソースを準備\n",
        "\n",
        "今回はウィキペディア上にある「ONE PIECE」のページをデータソースとして、それに関連する質問をしていきたいと思います。\n",
        "\n",
        "* https://ja.m.wikipedia.org/wiki/ONE_PIECE\n",
        "\n",
        "今回はウェブページのテキストだけを抽出してくれる `trafilatura` というライブラリを用いましたが、Langchain内にもこれ用の `BSHTMLLoader` というのがあるようです。まだ試せていません。"
      ],
      "metadata": {
        "id": "rwDtQT1oqqDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BSHTMLLoaderを使う場合のサンプルコード\n",
        "# https://python.langchain.com/docs/modules/data_connection/document_loaders/html から引用。\n",
        "# from langchain.document_loaders import BSHTMLLoader\n",
        "\n",
        "# loader = BSHTMLLoader(\"example_data/fake-content.html\")\n",
        "# data = loader.load()\n",
        "# data"
      ],
      "metadata": {
        "id": "DrKZo9g4jg8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trafilatura import fetch_url, extract\n",
        "\n",
        "url = \"https://ja.m.wikipedia.org/wiki/ONE_PIECE\"\n",
        "filename = 'textfile.txt'\n",
        "\n",
        "document = fetch_url(url)\n",
        "text = extract(document)\n",
        "print(text[:1000])\n",
        "\n",
        "with open(filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "OjHyDNg9iPYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "抽出したテキストをテキストファイルに保存出来ました。このテキストファイルをLangchainのTextSplitterを使って小口のチャンクに切っていきます。\n",
        "\n",
        "こうして生成したチャンクからembeddingを生成し、質問のembeddingに一番近いトップｋのチャンクを抽出。そのテキストをプロンプト内に突っ込み、質問と同時にLLMに投げて回答を得る。といった流れとなります。\n",
        "\n",
        "私は少なくともそういう理解です。"
      ],
      "metadata": {
        "id": "m87JvJNtrhjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(filename, encoding='utf-8')\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "print(len(texts))"
      ],
      "metadata": {
        "id": "O8XrzuC48-fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "どんな構造をしているのか知るために、何個か見てみましょう。"
      ],
      "metadata": {
        "id": "M1Vm3O_ssRbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts[30:33]"
      ],
      "metadata": {
        "id": "tvao6Dt7eXCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddingの生成とFAISSを使ったベクトルDBの用意\n",
        "\n",
        "小口のチャンクに切ったテキストを、Embeddingモデルを使ってembeddingに変換していきます。テキストの類似性をもとに検索をできるようにするためです。\n",
        "\n",
        "Embeddingの生成には `intfloat/multilingual-e5-large` を使います。\n",
        "\n",
        "ベクトルDBには `FAISS` のライブラリを用います。（今回はGPUのある環境で走らせてみているため、 faiss-gpu をロードしています。）"
      ],
      "metadata": {
        "id": "vqbGRqeGsUPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# 一番類似するチャンクをいくつロードするかを変数kに設定出来ます。\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "UgTRglxF9DEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回の環境ではembeddingを用意するのに25秒ほどかかりました。"
      ],
      "metadata": {
        "id": "R3NlvklczFgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの用意\n",
        "\n",
        "今回はElyza-7b-instructを用います。\n",
        "\n",
        "今回はColabのT4でも問題なく実行できるよう、BitsandBytesで4bitに量子化したものをロードします。"
      ],
      "metadata": {
        "id": "xiJfjjx1yBRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ").eval()"
      ],
      "metadata": {
        "id": "23g2UJetyHC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に、Elyza-7b-instruct用のプロンプトテンプレートを用意します。"
      ],
      "metadata": {
        "id": "Gvdx0XjhzTdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"参考情報を元に、ユーザーからの質問にできるだけ正確に答えてください。\"\n",
        "text = \"{context}\\n質問: {question}\"\n",
        "template = \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
        "    bos_token=tokenizer.bos_token,\n",
        "    b_inst=B_INST,\n",
        "    system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
        "    prompt=text,\n",
        "    e_inst=E_INST,\n",
        ")"
      ],
      "metadata": {
        "id": "h9aWJ-scyKBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMとChainの指定"
      ],
      "metadata": {
        "id": "pFsez5nts9nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        ")\n",
        "PROMPT = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template_format=\"f-string\"\n",
        ")\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=HuggingFacePipeline(\n",
        "        pipeline=pipe,\n",
        "        model_kwargs=dict(\n",
        "            temperature=0.0,\n",
        "            do_sample=True,\n",
        "            max_length=512,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "    ),\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "dwCNAHhycztp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## お試し\n",
        "\n",
        "要約質問ができる状態が整いました。\n",
        "最初にRAGを使わずにLLMに質問をし、その後にRAGを使って生成してみて差分を比較してみましょう。"
      ],
      "metadata": {
        "id": "7QTttq52tRwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = template.format(context='', question='ニコ・ロビンの職業は何ですか？')\n",
        "inputs = tokenizer(inputs, return_tensors='pt').to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)\n",
        "output"
      ],
      "metadata": {
        "id": "XrgIqlNnzm9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa(\"ニコ・ロビンの職業は何ですか？\")\n",
        "print('回答:', result['result'])\n",
        "print('='*10)\n",
        "print('ソース:', result['source_documents'])"
      ],
      "metadata": {
        "id": "Mi0Wcc6ad4FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = template.format(context='', question='エネルは何者ですか？')\n",
        "inputs = tokenizer(inputs, return_tensors='pt').to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)\n",
        "output"
      ],
      "metadata": {
        "id": "TygOY0yO3SBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa(\"エネルは何者ですか？\")\n",
        "print('回答:', result['result'])\n",
        "print('='*10)\n",
        "print('ソース:', result['source_documents'])"
      ],
      "metadata": {
        "id": "MIgAIwW_d5bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = template.format(context='', question='チョッパーの特殊能力は何ですか？')\n",
        "inputs = tokenizer(inputs, return_tensors='pt').to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)\n",
        "output"
      ],
      "metadata": {
        "id": "y0Sf2Hbm39Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa(\"チョッパーの特殊能力は何ですか？\")\n",
        "print('回答:', result['result'])\n",
        "print('='*10)\n",
        "print('ソース:', result['source_documents'])"
      ],
      "metadata": {
        "id": "KclpqfLUwkwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = template.format(context='', question=\"「ONE PIECE」とは作品の中で何を指していますか？\")\n",
        "inputs = tokenizer(inputs, return_tensors='pt').to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)\n",
        "output"
      ],
      "metadata": {
        "id": "NftahnE44MVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa(\"「ONE PIECE」とは作品の中で何を指していますか？\")\n",
        "print('回答:', result['result'])\n",
        "print('='*10)\n",
        "print('ソース:', result['source_documents'])"
      ],
      "metadata": {
        "id": "priK5mYmt3q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 結論\n",
        "\n",
        "RAGにより回答の質が全体的にかなり上がったことが確認できました。\n",
        "\n",
        "余談：最後の質問に対するGPT-4のRAGなしでの回答は下記でした。流石ですね:\n",
        "\n",
        "`\n",
        "「ONE PIECE」は、日本の漫画家尾田栄一郎（Eiichiro Oda）によって作られた漫画およびアニメ作品であり、その中で「One Piece」とは、伝説的な海賊ゴール・D・ロジャーが残したとされる、世界最大の財宝を指します。この財宝は、最も危険で未知な海域である「偉大なる航路（Grand Line）」の最後にある「ラフテル」という島に隠されているとされています。\n",
        "`"
      ],
      "metadata": {
        "id": "TMzE0qqA4oPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 参考\n",
        "\n",
        "こちらのノートブックを作成するにあたり、下記を参考にさせていただいております。\n",
        "\n",
        "* [alfredplpl/RetrievalQA.py](https://gist.github.com/alfredplpl/57a6338bce8a00de9c9d95bbf1a6d06d)\n",
        "* [Langchain Docs](https://python.langchain.com/docs/get_started/introduction)\n",
        "* [Wikipedia「ONE_PIECE」](https://ja.m.wikipedia.org/wiki/ONE_PIECE)\n",
        "\n"
      ],
      "metadata": {
        "id": "peRYCXDh5s77"
      }
    }
  ]
}