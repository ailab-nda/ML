{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMi+0rYtbAWOdxMfcBnlXG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/NLP/blob/main/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNwy3LZTjTPW"
      },
      "source": [
        "## GTP-2 による文書生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScPiJ0zCjuzK"
      },
      "source": [
        "### 準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX0jK3ZOSKTc"
      },
      "source": [
        "# ソースからのHuggingface Transformersのインストール\n",
        "!git clone https://github.com/huggingface/transformers -b v4.4.2\n",
        "!pip install -e transformers\n",
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_vKnWLAj4i0"
      },
      "source": [
        "### 動作確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqrPjJZoj_mq"
      },
      "source": [
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-small\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXVry2e9k-Fn"
      },
      "source": [
        "# 文書生成\n",
        "input = tokenizer.encode(\"みなさん、こんにちは。\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=3)\n",
        "sentences = tokenizer.batch_decode(output)\n",
        "for i in sentences:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd80SDxFkZ9G"
      },
      "source": [
        "### ファインチューニング"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G18S00TsTGKC"
      },
      "source": [
        "%%time\n",
        "!rm -r output\n",
        "# ファインチューニングの実行\n",
        "!python ./transformers/examples/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-small \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=10 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --fp16 \\\n",
        "    --output_dir=output/ \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFejgrTaU1B6"
      },
      "source": [
        "# モデルの準備\n",
        "model2 = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "\n",
        "# 推論\n",
        "input = tokenizer.encode(\"みなさん、こんにちは。\", return_tensors=\"pt\")\n",
        "output2 = model2.generate(input, do_sample=True, max_length=100, num_return_sequences=1)\n",
        "print(tokenizer.batch_decode(output2))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}