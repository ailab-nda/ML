{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "interpreter": {
      "hash": "c6b05b93daf3f9d51f6f9bad260edf5badcb96fbb82eb040035f228177e317a6"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit ('3.9.6': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "name": "TweetAnalysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/ML/blob/main/TweetAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c8e6629-408c-4672-bb5c-63c3f611174b"
      },
      "source": [
        "# 演習III 第４回 ツイッターテキスト解析"
      ],
      "id": "3c8e6629-408c-4672-bb5c-63c3f611174b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81a68669-906f-4364-a00e-53226a5ebf90"
      },
      "source": [
        "参考：https://akitoshiblogsite.com/tweepy-basic-functions/"
      ],
      "id": "81a68669-906f-4364-a00e-53226a5ebf90"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "864b15b3"
      },
      "source": [
        "必要なソフトウェアのインストール"
      ],
      "id": "864b15b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2128b87a"
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7\n",
        "!apt-get -y install fonts-ipafont-gothic"
      ],
      "id": "2128b87a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d53f7ddb-42c6-4081-b7a2-859d5e02823d"
      },
      "source": [
        "必要なライブラリのインストール"
      ],
      "id": "d53f7ddb-42c6-4081-b7a2-859d5e02823d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d98732b9-1901-4093-aae0-fb2e37ae04fe"
      },
      "source": [
        "!pip install tweepy mecab-python3 wordcloud oseti japanize-matplotlib"
      ],
      "id": "d98732b9-1901-4093-aae0-fb2e37ae04fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb2ea599-3e8a-454b-8ac3-f3e3d3cf9c2d"
      },
      "source": [
        "ライブラリのインポートを行います。"
      ],
      "id": "fb2ea599-3e8a-454b-8ac3-f3e3d3cf9c2d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51196983-f9a7-4964-9a9a-1b736f240b3e"
      },
      "source": [
        "import tweepy\n",
        "import MeCab\n",
        "import csv\n",
        "import json\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import oseti\n",
        "import collections\n",
        "import codecs\n",
        "import random"
      ],
      "id": "51196983-f9a7-4964-9a9a-1b736f240b3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bade572-79dd-464c-89fc-deba2c872af2"
      },
      "source": [
        "## MeCab のテスト\n",
        "\n",
        "文章を単語ごとに区切りたいので MeCab というソフトを使います。"
      ],
      "id": "5bade572-79dd-464c-89fc-deba2c872af2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyVqPDzToP5o"
      },
      "source": [
        "mecab = MeCab.Tagger(\"-Ochasen\") \n",
        "malist = mecab.parse(\"すもももももももものうち\")\n",
        "print(malist)"
      ],
      "id": "qyVqPDzToP5o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtVMRdqpoP5p"
      },
      "source": [
        "## Tweet の取得\n",
        "\n",
        "Twitter API を利用します（利用には申請が必要）。"
      ],
      "id": "YtVMRdqpoP5p"
    },
    {
      "cell_type": "code",
      "source": [
        "import config # config.py\n",
        "CK = config.CONSUMER_KEY\n",
        "CS = config.CONSUMER_SECRET\n",
        "AT = config.ACCESS_TOKEN\n",
        "AS = config.ACCESS_TOKEN_SECRET\n",
        "BT = config.BEARER_TOKEN"
      ],
      "metadata": {
        "id": "Od1kGGxo-gEF"
      },
      "id": "Od1kGGxo-gEF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "169ae03d-9503-42d9-a21c-c252c25829c5"
      },
      "source": [
        "auth = tweepy.OAuthHandler(CK, CS)\n",
        "auth.set_access_token(AT, AS)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ],
      "id": "169ae03d-9503-42d9-a21c-c252c25829c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c38b52fd-8d57-45c1-9c22-94a21145020a"
      },
      "source": [
        "### *取得例１：特定ユーザの* Tweet を取得\n",
        "防大公式のツイートをツイートを tweets 変数に格納します。"
      ],
      "id": "c38b52fd-8d57-45c1-9c22-94a21145020a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpFuUPreoP5q"
      },
      "source": [
        "tweets = tweepy.Cursor(api.user_timeline, screen_name=\"ndanyusi\", tweet_mode = 'extended')"
      ],
      "id": "tpFuUPreoP5q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECW4OXjnoP5r"
      },
      "source": [
        "### 取得例２：指定したキーワードを含む Tweet を取得\n",
        "「防衛大」というワードを含むツイートを tweets 変数に格納します。"
      ],
      "id": "ECW4OXjnoP5r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIwO06FCoP5r"
      },
      "source": [
        "tweets = tweepy.Cursor(api.search, q=\"防衛大 exclude:retweets\", include_entities = True, \n",
        "tweet_mode = 'extended', lang = 'ja', result_type = 'mixed')"
      ],
      "id": "EIwO06FCoP5r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shgY9HWLoP5r"
      },
      "source": [
        "### 取得結果の表示\n",
        "tweets 変数（上記）に格納されたツイートを表示します。"
      ],
      "id": "shgY9HWLoP5r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-60XQMa3oP5s"
      },
      "source": [
        "for tweet_json in tweets.items(3):\n",
        "    #print(tweet_json)\n",
        "    tweet = tweet_json._json\n",
        "    print(tweet['full_text'].replace('\\n',' '))\n",
        "    print(\"=================================\")"
      ],
      "id": "-60XQMa3oP5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-rPe_uJ8y1h"
      },
      "source": [
        "### 結果をデータフレームに格納"
      ],
      "id": "r-rPe_uJ8y1h"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e0210b5"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "for tweet_json in tweets.items(100):\n",
        "    tweet = tweet_json._json\n",
        "    df = pd.concat([df, pd.json_normalize(tweet)], ignore_index=True)\n",
        "df = df.replace('\\n','', regex=True)\n",
        "df"
      ],
      "id": "8e0210b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9932af"
      },
      "source": [
        "### 結果をファイルに保存"
      ],
      "id": "ec9932af"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a3ec664"
      },
      "source": [
        "df2 = df[['created_at', 'user.name', 'full_text', 'favorite_count', 'retweet_count', 'user.followers_count']]\n",
        "df2.to_csv('tweets.csv', index=False, encoding='shift-jis', errors='ignore')"
      ],
      "id": "2a3ec664",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da2dc6f9"
      },
      "source": [
        "### 結果をファイルから読み込む"
      ],
      "id": "da2dc6f9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90110c76"
      },
      "source": [
        "df2 = pd.read_csv('tweets.csv', encoding='cp932')\n",
        "df2"
      ],
      "id": "90110c76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEejaPPOoP5v"
      },
      "source": [
        "## MeCab を使った前処理"
      ],
      "id": "uEejaPPOoP5v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvlMmlqjoP5v"
      },
      "source": [
        "### 分かち書き、品詞の抜き出し"
      ],
      "id": "pvlMmlqjoP5v"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDbUPCa6oP5v"
      },
      "source": [
        "words = []\n",
        "for tweet in df['full_text']:\n",
        "    node = mecab.parseToNode(tweet)\n",
        "    while node:\n",
        "        word = node.surface\n",
        "        word_type = node.feature.split(\",\")[0]\n",
        " \n",
        "        # \"名詞\", \"動詞\", \"形容詞\", \"副詞\"の中で選択したものを抽出\n",
        "        if word_type in [\"名詞\", \"動詞\", \"形容詞\"]:\n",
        "            words.append(word)\n",
        "        node = node.next\n"
      ],
      "id": "SDbUPCa6oP5v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1kJm8ZboP5w"
      },
      "source": [
        "## 分析例１：WordCloud として表示"
      ],
      "id": "Q1kJm8ZboP5w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqi6QRF3solk"
      },
      "source": [
        "wakati_text = \" \".join(words)\n",
        "fpath = '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf'  # 日本語フォント指定\n",
        "stop_words = ['https', 't', 'co', 'RT']\n",
        "wordcloud = WordCloud(\n",
        "    font_path=fpath,\n",
        "    width=900, height=600,   # default width=400, height=200\n",
        "    background_color=\"white\",   # default=”black”\n",
        "    stopwords=set(stop_words),\n",
        "    max_words=500,   # default=200\n",
        "    min_font_size=4,   #default=4\n",
        "    collocations = False   #default = True\n",
        "    ).generate(wakati_text)\n",
        " \n",
        "plt.figure(figsize=(15,12))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"word_cloud.png\")\n",
        "plt.show()"
      ],
      "id": "xqi6QRF3solk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "802816e5"
      },
      "source": [
        "## 分析例２：ポジ・ネガ分析"
      ],
      "id": "802816e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc35ae70"
      },
      "source": [
        "analyzer = oseti.Analyzer()\n",
        "print(analyzer.analyze_detail(\"最後まで希望を捨てちゃいかん。あきらめたら、そこで試合終了だよ。\"))\n",
        "print(analyzer.analyze_detail(\"認めたくないものだな。自分自身の、若さゆえの過ちというものを。\"))"
      ],
      "id": "cc35ae70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLExLihAoP5x"
      },
      "source": [
        "ave_senti = []\n",
        "for tweet in df['full_text']:\n",
        "    print(tweet)\n",
        "    senti = analyzer.analyze(tweet)\n",
        "    print(analyzer.analyze_detail(tweet), np.mean(senti))\n",
        "    ave_senti.append(np.mean(senti))\n",
        "ave_senti"
      ],
      "id": "eLExLihAoP5x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "056d2d8d"
      },
      "source": [
        "hist, bins = np.histogram(ave_senti, bins=4)\n",
        "plt.pie(hist, labels=['ネガ', 'ややネガ', 'ややポジ', 'ポジ'], counterclock=False, startangle=90)"
      ],
      "id": "056d2d8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4hQwIaGoP50"
      },
      "source": [
        "# 課題\n",
        "各自のテーマで Tweet を分析せよ。"
      ],
      "id": "T4hQwIaGoP50"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpa_-IcW9gvo"
      },
      "source": [
        ""
      ],
      "id": "Mpa_-IcW9gvo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f98e175c"
      },
      "source": [
        "## おまけ：感情分析（Transformer 版）"
      ],
      "id": "f98e175c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa3cb339"
      },
      "source": [
        "!pip install transformers fugashi ipadic"
      ],
      "id": "aa3cb339",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d44c78fc"
      },
      "source": [
        "from transformers import pipeline \n",
        "from transformers import AutoModelForSequenceClassification \n",
        "from transformers import BertJapaneseTokenizer "
      ],
      "id": "d44c78fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c27ccfe"
      },
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('daigo/bert-base-japanese-sentiment') \n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking') \n",
        "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer) "
      ],
      "id": "1c27ccfe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bec0b5d0"
      },
      "source": [
        "print(nlp(\"最後まで希望を捨てちゃいかん。あきらめたら、そこで試合終了だよ。\"))\n",
        "print(nlp(\"認めたくないものだな。自分自身の、若さゆえの過ちというものを。\"))"
      ],
      "id": "bec0b5d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iFAFOe4oP50"
      },
      "source": [
        "sentiments = []\n",
        "for tweet in df['full_text']:\n",
        "    print(tweet)\n",
        "    senti = nlp(tweet)\n",
        "    print(senti)\n",
        "    sentiments.append(senti[0]['score'])\n",
        "print(sentiments)\n",
        "print(np.mean(sentiments))"
      ],
      "id": "9iFAFOe4oP50",
      "execution_count": null,
      "outputs": []
    }
  ]
}
