{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/ML/blob/main/Nomusan_Ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ノムさん度の判定**\n",
        "# <img src='https://ollama.com/public/ollama.png' alt=\"Ollama\"/>\n",
        "Huggingface のモデルだと時間がかかるので、別のLLMを用意しました。"
      ],
      "metadata": {
        "id": "nE_kmrLel_y4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzG-3nviVQTM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Install components\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "!pip install ollama\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Start server\n",
        "import subprocess\n",
        "proccess = subprocess.Popen(['ollama', 'serve'])"
      ],
      "metadata": {
        "id": "0oEhDdPXVamH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select your model\n",
        "model = \"qwen2.5:7b\" # @param {\"type\":\"string\"}\n",
        "!ollama pull {model}"
      ],
      "metadata": {
        "id": "O5toc_VkVffm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Interacting with the model\n",
        "question = \"あなたは誰ですか？\" # @param {\"type\":\"string\"}\n",
        "from IPython.display import display, Markdown\n",
        "import ollama\n",
        "response = ollama.chat(model, messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': question,\n",
        "  },\n",
        "])\n",
        "print(response['message']['content'])\n",
        "#display(Markdown(response['message']['content']))"
      ],
      "metadata": {
        "id": "n0CMtBUqV75y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JUDGE_SYSTEM = \"\"\"あなたは、野村克也監督（ID野球）の思想・配球観・攻撃采配に精通した “審判AI（LLM-as-a-Judge）”です。\n",
        "\n",
        "これから与える「状況」と「4つの回答（A/B/C/D）」を読み、以下の **5観点** で **0〜10点の整数値** で採点してください。\n",
        "\n",
        "【評価観点】\n",
        "\n",
        "1. nomura_like（野村監督らしさ）\n",
        "  - ID野球の基礎（相性分析、確率思考、配球読み、データ重視、送りバントの慎重運用など）に沿うか\n",
        "  - 「状況に応じた合理性」や「情報の非対称性の活用」が表現されているか\n",
        "  - 野村監督の著書・インタビューに一致する思考様式か\n",
        "\n",
        "2. tactical_quality（戦術としての期待値・合理性）\n",
        "  - 回・アウトカウント・走者・打順・カウント・点差・相手投手の特徴などから見て得点期待値の高い采配になっているか\n",
        "  - 作戦が矛盾していないか、過剰リスクや無意味なギャンブルがないか\n",
        "\n",
        "3. format_quality（フォーマット遵守度）\n",
        "  - 「作戦」「根拠（2文以内）」「具体的なサイン（1文）」の3行構成になっているか\n",
        "  - 300〜400文字の制限内に収まっているか\n",
        "  - 誤字・欠落・論理破綻がないか\n",
        "\n",
        "4. explanation_clarity（説明の明快さ・理解性）\n",
        "  - 結論と根拠の関係が明確で、論理的に筋が通っているか\n",
        "  - 説明が抽象的すぎず、具体性と状況対応性があるか\n",
        "  - 読み手が納得しやすい明快なロジックになっているか\n",
        "\n",
        "5. instruction_following（指示遵守度）\n",
        "  - プロンプトで指定された形式（行数・文字数・構成）を厳密に守っているか\n",
        "  - 禁止表現（野球以外の話題、冗長な一般論、口語表現など）を避けているか\n",
        "  - 出力全体から設計意図に従う姿勢が読み取れるか\n",
        "\n",
        "【重要ルール】\n",
        "- A/B/C/D のどれがどのモデルのものかは知らされていません。内容のみで判断してください。\n",
        "- 採点はすべて 0〜10 の整数。\n",
        "- 最も優れている回答を A/B/C/D のいずれか1つ選び、best_answer に記述。\n",
        "- 必ずJSONのみを返し、説明・コメントは一切書かないこと。\n",
        "- JSONは、必ず以下の形式で、**`\"A\"`, `\"B\"`, `\"C\"`, `\"D\"` をトップレベルのキーとして含めてください。**\n",
        "\n",
        "【出力形式】\n",
        "{\n",
        "  \"A\":{\"nomura_like\":0,\"tactical_quality\":0,\"format_quality\":0,\"explanation_clarity\":0,\"instruction_following\":0},\n",
        "  \"B\":{\"nomura_like\":0,\"tactical_quality\":0,\"format_quality\":0,\"explanation_clarity\":0,\"instruction_following\":0},\n",
        "  \"C\":{\"nomura_like\":0,\"tactical_quality\":0,\"format_quality\":0,\"explanation_clarity\":0,\"instruction_following\":0},\n",
        "  \"D\":{\"nomura_like\":0,\"tactical_quality\":0,\"format_quality\":0,\"explanation_clarity\":0,\"instruction_following\":0},\n",
        "  \"best_answer\":\"A\"\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dEM5sO0al95c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_judge_local(prompt, max_new_tokens=512):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": JUDGE_SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    outputs = ollama.chat(model, messages=messages)\n",
        "    return outputs['message']['content']"
      ],
      "metadata": {
        "id": "x_omrJwUnWzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_judge_local(prompt, max_new_tokens=512):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": JUDGE_SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.0,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # JSON開始位置から切り出し（安全策）\n",
        "    json_start = decoded.rfind(\"{\")\n",
        "    return decoded[json_start:]"
      ],
      "metadata": {
        "id": "18aJa-i4l_Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "FILES = {\n",
        "    \"model_only\": \"/content/results_野村_model_only.json\",\n",
        "    \"sft_only\": \"/content/results_野村_finetuning.json\",\n",
        "    \"rule_rag\": \"/content/results_野村_lule.json\",\n",
        "    \"sft_rule\": \"/content/results_野村_finetuning_RAGlule.json\",\n",
        "}\n",
        "\n",
        "def load_results(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return {x[\"id\"]: x[\"output\"] for x in json.load(f)}\n",
        "\n",
        "loaded = {k: load_results(v) for k, v in FILES.items()}\n",
        "ids = sorted(set.intersection(*[set(v.keys()) for v in loaded.values()]))\n",
        "\n",
        "print(\"評価対象件数:\", len(ids))"
      ],
      "metadata": {
        "id": "emMhew7TmF3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_judge_prompt(scenario, answers):\n",
        "    return f\"\"\"\n",
        "【試合状況】\n",
        "{scenario}\n",
        "\n",
        "【回答A】\n",
        "{answers[\"A\"]}\n",
        "\n",
        "【回答B】\n",
        "{answers[\"B\"]}\n",
        "\n",
        "【回答C】\n",
        "{answers[\"C\"]}\n",
        "\n",
        "【回答D】\n",
        "{answers[\"D\"]}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0N2FZpTHmIyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in tqdm(ids):\n",
        "    scenario = loaded[\"model_only\"][i]  # 入力は共通想定\n",
        "\n",
        "    answers = {\n",
        "        \"A\": loaded[\"model_only\"][i],\n",
        "        \"B\": loaded[\"sft_only\"][i],\n",
        "        \"C\": loaded[\"rule_rag\"][i],\n",
        "        \"D\": loaded[\"sft_rule\"][i],\n",
        "    }\n",
        "\n",
        "    prompt = build_judge_prompt(scenario, answers)\n",
        "    judge_json = call_judge_local(prompt)\n",
        "\n",
        "    # Add the model_map to the results list\n",
        "    results.append({\n",
        "        \"id\": i,\n",
        "        \"judge_raw\": judge_json,\n",
        "        \"map\": {\n",
        "            \"A\": \"model_only\",\n",
        "            \"B\": \"sft_only\",\n",
        "            \"C\": \"rule_rag\",\n",
        "            \"D\": \"sft_rule\",\n",
        "        }\n",
        "    })"
      ],
      "metadata": {
        "id": "T3CbCY6PmS1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def safe_json_load(text):\n",
        "    \"\"\"\n",
        "    LLM出力から有効なJSONオブジェクトを抽出し、ロードを試みる。\n",
        "    LLMが完全なJSONオブジェクトではないもの（例えば、一部のキーが欠落している、\n",
        "    または余分なテキストが含まれている）を出力する可能性があるため、\n",
        "    より堅牢な抽出ロジックを使用する。\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    # 最も外側のJSONオブジェクトを探す\n",
        "    match = re.search(r\"(\\{.*\\})\", text, re.DOTALL)\n",
        "    if not match:\n",
        "        return None\n",
        "\n",
        "    potential_json_str = match.group(1)\n",
        "\n",
        "    try:\n",
        "        # まず、そのままロードを試みる\n",
        "        return json.loads(potential_json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        # ロードに失敗した場合、特定のLLMの出力形式に対応するためのヒューリスティックを試す\n",
        "        # 観察された不正な形式: `{\"key\": value}, \"best_answer\":\"X\"`\n",
        "        last_brace_comma_index = potential_json_str.rfind(\"},\")\n",
        "        if last_brace_comma_index != -1:\n",
        "            first_part = potential_json_str[:last_brace_comma_index]\n",
        "            remaining_part = potential_json_str[last_brace_comma_index+2:].strip()\n",
        "\n",
        "            # Now remaining_part might be something like '\"best_answer\":\"A\"}'\n",
        "            # We need to extract the key-value pair and ensure it's valid.\n",
        "            if remaining_part.endswith('}'):\n",
        "                extracted_second_part = remaining_part[:-1].strip() # Remove the final '}'\n",
        "            else:\n",
        "                extracted_second_part = remaining_part.strip()\n",
        "\n",
        "            if extracted_second_part.startswith('\"best_answer\":'):\n",
        "                repaired_json_str = f\"{first_part}, {extracted_second_part}}}\"\n",
        "                try:\n",
        "                    return json.loads(repaired_json_str)\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "\n",
        "        # その他のJSONDecodeErrorの場合、または修復が失敗した場合\n",
        "        return None"
      ],
      "metadata": {
        "id": "Uec0Hz13mXlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "score_cols = [\n",
        "    \"nomura_like\",\n",
        "    \"tactical_quality\",\n",
        "    \"format_quality\",\n",
        "    \"explanation_clarity\",\n",
        "    \"instruction_following\"\n",
        "]\n",
        "\n",
        "for r in results:\n",
        "    j = safe_json_load(r[\"judge_raw\"])\n",
        "    if j is None:\n",
        "        print(f\"Skipping entry {r['id']} due to JSON parsing error.\")\n",
        "        continue\n",
        "\n",
        "    model_map = r[\"map\"]\n",
        "\n",
        "    # Check if 'best_answer' exists in the parsed JSON\n",
        "    if \"best_answer\" not in j:\n",
        "        print(f\"Skipping entry {r['id']} as 'best_answer' key is missing in LLM output: {j}\")\n",
        "        continue\n",
        "\n",
        "    best_k = j[\"best_answer\"]\n",
        "\n",
        "    for k in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "        row_data = {\n",
        "            \"id\": r[\"id\"],\n",
        "            \"model_key\": model_map[k],\n",
        "        }\n",
        "        if k == best_k:\n",
        "            # For the best answer, extract its specific scores from j[best_k]\n",
        "            # Ensure j[best_k] exists and is a dictionary\n",
        "            if isinstance(j.get(best_k), dict):\n",
        "                for col in score_cols:\n",
        "                    row_data[col] = j[best_k].get(col, 0)\n",
        "            else:\n",
        "                # If the best_k entry itself is malformed, assign 0s\n",
        "                for col in score_cols:\n",
        "                    row_data[col] = 0\n",
        "        else:\n",
        "            # For other answers, assign 0 for all scores\n",
        "            for col in score_cols:\n",
        "                row_data[col] = 0\n",
        "        rows.append(row_data)\n",
        "\n",
        "# Create DataFrame from rows (now populated correctly)\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Ensure score columns are numeric\n",
        "for c in score_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Calculate mean only for the identified score columns\n",
        "mean_scores = df[score_cols].mean().round(3)\n",
        "mean_scores.to_csv(\n",
        "    \"/content/judge_mean_scores.csv\",\n",
        "    encoding=\"utf-8-sig\"\n",
        ")\n",
        "\n",
        "print(\"✅ 平均スコアCSV 出力完了\")\n",
        "display(mean_scores)\n"
      ],
      "metadata": {
        "id": "R_D4n9xAmdUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows"
      ],
      "metadata": {
        "id": "beyM2LjGFLQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "HS_CpCBZFJI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.colab import sheets\n",
        "sheet = sheets.InteractiveSheet(df=df)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "DBptluBJFeTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "j[best_k]"
      ],
      "metadata": {
        "id": "rpU08WuUEsaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j"
      ],
      "metadata": {
        "id": "7TPMw-pmE82S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "score_cols = [\n",
        "    \"nomura_like\",\n",
        "    \"tactical_quality\",\n",
        "    \"format_quality\",\n",
        "    \"explanation_clarity\",\n",
        "    \"instruction_following\"\n",
        "]\n",
        "\n",
        "# Create DataFrame from rows (now populated correctly)\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Ensure score columns are numeric\n",
        "for c in score_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Calculate mean only for the identified score columns\n",
        "mean_scores = df[score_cols].mean().round(3)\n",
        "mean_scores.to_csv(\n",
        "    \"/content/judge_mean_scores.csv\",\n",
        "    encoding=\"utf-8-sig\"\n",
        ")\n",
        "\n",
        "print(\"✅ 平均スコアCSV 出力完了\")\n",
        "display(mean_scores)\n"
      ],
      "metadata": {
        "id": "fdvrx4TQmixV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "ah-oApB3mpFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_cols = [\n",
        "    \"nomura_like\",\n",
        "    \"tactical_quality\",\n",
        "    \"format_quality\",\n",
        "    \"explanation_clarity\",\n",
        "    \"instruction_following\"\n",
        "]\n",
        "\n",
        "for c in score_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")"
      ],
      "metadata": {
        "id": "wTwc7iTKmrg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "qeUdwgWWmuqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(rows))"
      ],
      "metadata": {
        "id": "wDxNdDdFmwXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# スコア列\n",
        "score_cols = [\n",
        "    \"nomura_like\",\n",
        "    \"tactical_quality\",\n",
        "    \"format_quality\",\n",
        "    \"explanation_clarity\",\n",
        "    \"instruction_following\"\n",
        "]\n",
        "\n",
        "# 数値化（ここが肝）\n",
        "for c in score_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "display(df.head())\n",
        "print(df.dtypes)\n",
        "\n",
        "# 平均算出\n",
        "mean_scores = df[score_cols].mean().round(3)\n",
        "\n",
        "mean_scores.to_csv(\n",
        "    \"/content/judge_mean_scores.csv\",\n",
        "    encoding=\"utf-8-sig\"\n",
        ")\n",
        "\n",
        "print(\"✅ 平均スコアCSV 出力完了\")\n",
        "display(mean_scores)"
      ],
      "metadata": {
        "id": "sWKWCWljmz4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLVPdmp2AZ83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}