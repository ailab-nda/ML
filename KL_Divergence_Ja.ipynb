{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/ML/blob/main/KL_Divergence_Ja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KLダイバージェンスを理解する\n",
        "\n",
        "https://www.kaggle.com/code/meaninglesslives/understanding-kl-divergence/notebook を DeepL で翻訳したものです。"
      ],
      "metadata": {
        "id": "yO7hj1rnQrmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variational Auto Encoderの論文を読んで、KLダイバージェンスに興味を持った。そこで、より直感的に理解するために調べてみることにした。\n",
        "\n",
        "KLダイバージェンスとは、ある確率分布（$P$）が2つ目の確率分布（$Q$）とどの程度異なるかを示す尺度である。もし2つの分布が同じなら、KL発散は0になるはずです。したがって、KL発散を最小化することで、$P$に近似する2番目の分布($Q$)のパラメータを見つけることができます。\n",
        "\n",
        "この投稿では、分布$P$（2つのガウシアンの和）を、もう1つのガウシアン分布$Q$とのKL発散を最小化することで近似してみる。\n"
      ],
      "metadata": {
        "id": "P__xelarQjH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Libraries"
      ],
      "metadata": {
        "id": "DuHUc2YxQjH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import grad\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for animation\n",
        "%matplotlib inline\n",
        "import matplotlib.animation\n",
        "from IPython.display import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from typing import Dict, List, Tuple"
      ],
      "metadata": {
        "trusted": true,
        "id": "6ymNXq26QjH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing Gaussians"
      ],
      "metadata": {
        "id": "zcrrKBBiQjIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorchは特定の種類のディストリビューションからサンプルを取得する簡単な方法を提供します。torch.distributesにはよく使われる分布がたくさんあります。\n",
        "まず、$$\\mu_{1}=-5, \\sigma_{1}=1$$と$$\\mu_{1}=10, ˶=1$$ の2つのガウス分布を作ってみましょう。"
      ],
      "metadata": {
        "id": "WRTdH-_8QjIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu1,sigma1 = -5,1\n",
        "mu2,sigma2 = 10,1\n",
        "\n",
        "gaussian1 = torch.distributions.Normal(mu1,sigma1)\n",
        "gaussian2 = torch.distributions.Normal(mu2,sigma2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5r0osCA8QjIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 健全性チェック\n",
        "期待されるパラメータを持つガウス分布かどうかを検証するために、いくつかのポイントで分布をサンプリングしてみましょう。"
      ],
      "metadata": {
        "id": "fhX5-ABzQjIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,4))\n",
        "x = torch.linspace(mu1-5*sigma1,mu1+5*sigma1,1000)\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(x.numpy(),gaussian1.log_prob(x).exp().numpy())\n",
        "plt.title(f'$\\mu={mu1},\\sigma={sigma1}$')\n",
        "\n",
        "x = torch.linspace(mu2-5*sigma2,mu2+5*sigma2,1000)\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(x.numpy(),gaussian2.log_prob(x).exp().numpy())\n",
        "plt.title(f'$\\mu={mu2},\\sigma={sigma2}$')\n",
        "\n",
        "plt.suptitle('Plotting the distributions')"
      ],
      "metadata": {
        "trusted": true,
        "id": "_zb1OJ_GQjIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の図は、分布が正しく構成されていることを示している。\n",
        "\n",
        "ガウシアンを加えて、新しい分布$P(x)$を生成してみよう。\n",
        "\n",
        "我々の目的は、この新しい分布\n",
        "$Q(x)$ を使って近似することである。分布$P(x)$と$Q(x)$の間のKLダイバージェンスを最小化することで、パラメータ $\\mu_{Q},\\sigma_{Q}$を求めよう。"
      ],
      "metadata": {
        "id": "hd5him5PQjID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,4))\n",
        "x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)\n",
        "px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(x.numpy(),px.numpy())\n",
        "plt.title('$P(X)$')"
      ],
      "metadata": {
        "trusted": true,
        "id": "ibG34fOlQjID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $Q(X)$ の構成\n",
        "\n",
        "$P(X)$を近似するためにガウス分布を使う。分布$P(x)$を最もよく近似する最適なパラメータはわからない。\n",
        "\n",
        "そこで、単純に$\\mu=0,\\sigma=1$とします。\n",
        "\n",
        "$P(x)$を近似しようとしている分布についての予備知識があるので、もっと良い値を選ぶこともできる。しかし、実際のシナリオではそうでないことがほとんどです。"
      ],
      "metadata": {
        "id": "LlKyeKh4QjID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu = torch.tensor([0.0])\n",
        "sigma = torch.tensor([1.0])\n",
        "\n",
        "plt.figure(figsize=(14,4))\n",
        "x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)\n",
        "Q = torch.distributions.Normal(mu,sigma) # this should approximate P, eventually :-)\n",
        "qx = Q.log_prob(x).exp()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(x.numpy(),qx.detach().numpy())\n",
        "plt.title('$Q(X)$')"
      ],
      "metadata": {
        "trusted": true,
        "id": "jK4FPIRAQjIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KL-Divergence\n",
        "$$D_{KL}(P(x)||Q(X)) = \\sum_{x \\in X} P(x) \\log(P(x) / Q(x))$$\n"
      ],
      "metadata": {
        "id": "bqyxUoHvQjIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pytorch での計算"
      ],
      "metadata": {
        "id": "1KJm6BAnQjIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PytorchはKL発散を計算する関数を提供しています。詳しくは[こちら](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.kl_div)を参照してください。\n",
        "\n",
        "注意すべき点は、与えられた入力には対数確率が含まれていることです。ターゲットは確率として（つまり対数を取らずに）与えられます。\n",
        "\n",
        "したがって、関数の第1引数はQ、第2引数はP（目標分布）となります。\n",
        "\n",
        "また、数値の安定性にも注意しなければならない。"
      ],
      "metadata": {
        "id": "H7zmMRImQjIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()\n",
        "qx = Q.log_prob(x).exp()\n",
        "F.kl_div(qx.log(),px)"
      ],
      "metadata": {
        "trusted": true,
        "id": "GCtOdQKFQjIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "発散は無限大 :-) この問題は、指数計算をしてから対数計算をしたときに発生すると思います。対数値を直接使うのは問題ないようです。"
      ],
      "metadata": {
        "id": "7Ogy_nC0QjIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()\n",
        "qx = Q.log_prob(x)\n",
        "F.kl_div(qx,px)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bA02YXSVQjIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_loss(px: torch.tensor, loss_fn: str, muq: float = 0.0, sigmaq: float = 1.0,\\\n",
        "                  subsample_factor:int = 3,mode:str = 'min') -> Tuple[float,float,List[int]]:\n",
        "\n",
        "    mu = torch.tensor([muq],requires_grad=True)\n",
        "    sigma = torch.tensor([sigmaq],requires_grad=True)\n",
        "\n",
        "    opt = torch.optim.Adam([mu,sigma])\n",
        "\n",
        "    loss_val = []\n",
        "    epochs = 10000\n",
        "\n",
        "    #required for animation\n",
        "    all_qx,all_mu = [],[]\n",
        "    subsample_factor = 3 #have to subsample to reduce memory usage\n",
        "\n",
        "    torch_loss_fn = getattr(F,loss_fn)\n",
        "    for i in range(epochs):\n",
        "        Q = torch.distributions.Normal(mu,sigma) # this should approximate P\n",
        "        if loss_fn!='kl_div': # we need to exponentiate q(x) for these and few other cases\n",
        "            qx = Q.log_prob(x).exp()\n",
        "            all_qx.append(qx.detach().numpy()[::subsample_factor])\n",
        "        else:\n",
        "            qx = Q.log_prob(x)\n",
        "            all_qx.append(qx.exp().detach().numpy()[::subsample_factor])\n",
        "\n",
        "        if mode=='min':\n",
        "            loss = torch_loss_fn(qx,px)\n",
        "        else:\n",
        "            loss = -torch_loss_fn(qx,px,dim=0)\n",
        "    #   backward pass\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        loss_val.append(loss.detach().numpy())\n",
        "        all_mu.append(mu.data.numpy()[0])\n",
        "\n",
        "\n",
        "        if i%(epochs//10)==0:\n",
        "            print('Epoch:',i,'Loss:',loss.data.numpy(),'mu',mu.data.numpy()[0],'sigma',sigma.data.numpy()[0])\n",
        "\n",
        "\n",
        "    print('Epoch:',i,'Loss:',loss.data.numpy(),'mu',mu.data.numpy()[0],'sigma',sigma.data.numpy()[0])\n",
        "\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.subplot(2,2,1)\n",
        "    plt.plot(loss_val)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel(f'{loss_fn} (Loss)')\n",
        "    plt.title(f'{loss_fn} vs epoch')\n",
        "\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.plot(all_mu)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('$\\mu$')\n",
        "    plt.title('$\\mu$ vs epoch')\n",
        "\n",
        "    return mu.data.numpy()[0],sigma.data.numpy()[0],all_qx"
      ],
      "metadata": {
        "trusted": true,
        "id": "mDgBLgqEQjIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)\n",
        "px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()\n",
        "mu,sigma,all_qx = optimize_loss(px, loss_fn='kl_div', muq = 0.0, sigmaq = 1.0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pdzujTVeQjIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_animation(x:torch.tensor,px:torch.tensor,all_qx:List,subsample_factor:int = 3,\\\n",
        "                     fn:str = 'anim_distr.gif') -> None:\n",
        "\n",
        "    # create a figure, axis and plot element\n",
        "    fig = plt.figure()\n",
        "    ax = plt.axes(xlim=(x.min(),x.max()), ylim=(0,0.5))\n",
        "    text = ax.text(3,0.3,0)\n",
        "    line1, = ax.plot([], [], color = \"r\")\n",
        "    line2, = ax.plot([], [], color = \"g\",alpha=0.7)\n",
        "\n",
        "    def animate(i):\n",
        "    #     non uniform sampling, interesting stuff happens fast initially\n",
        "        if i<75:\n",
        "            line1.set_data(x[::subsample_factor].numpy(),all_qx[i*50])\n",
        "            text.set_text(f'epoch={i*50}')\n",
        "            line2.set_data(x[::subsample_factor].numpy(),px.numpy()[::subsample_factor])\n",
        "        else:\n",
        "            line1.set_data(x[::subsample_factor].numpy(),all_qx[i*100])\n",
        "            text.set_text(f'epoch={i*100}')\n",
        "            line2.set_data(x[::subsample_factor].numpy(),px.numpy()[::subsample_factor])\n",
        "\n",
        "        return [line1,line2]\n",
        "\n",
        "    ani = matplotlib.animation.FuncAnimation(fig,animate,frames=100\n",
        "                                   ,interval=200, blit=True)\n",
        "\n",
        "    fig.suptitle(f'Minimizing the {fn[:-3]}')\n",
        "    ax.legend(['Approximation','Actual Distribution'])\n",
        "    # save the animation as gif\n",
        "    ani.save(fn, writer='imagemagick', fps=10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-XhvocZLQjIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% capture if you dont want to display the final image\n",
        "ani = create_animation(x,px,all_qx,fn='kl_div.gif')\n",
        "Image(\"kl_div.gif\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "tTlqxo7lQjIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "P$と$Q$の間の平均二乗距離を解いてみるとどうなるか見てみよう。"
      ],
      "metadata": {
        "trusted": true,
        "id": "uu5jlbqMQjIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)\n",
        "px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()\n",
        "mu,sigma,all_qx = optimize_loss(px, loss_fn='mse_loss', muq = 0.0, sigmaq = 1.0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "BcTzKIKeQjIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fn = 'mse_loss_mean0.gif'\n",
        "ani = create_animation(x,px,all_qx,fn=fn)\n",
        "Image(f\"{fn}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "PbAYmYwDQjIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果はKLダイバージェンスの場合とは大きく異なることがわかります。ガウシアンの1つに向かって収束しており、中間値はありません！\n",
        "\n",
        "また、$\\mu_{Q}$の初期値を変えて実験してみてください。10(2番目のガウスの平均)に近い値を選ぶと、それに向かって収束します。"
      ],
      "metadata": {
        "id": "fGddiVWvQjIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)\n",
        "px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()\n",
        "mu,sigma,all_qx = optimize_loss(px, loss_fn='mse_loss', muq = 5.0, sigmaq = 1.0)\n",
        "\n",
        "fn = 'mse_loss_mean5.gif'\n",
        "ani = create_animation(x,px,all_qx,fn=fn)\n",
        "Image(f\"../working/{fn}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Yhmga8hWQjIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1ロスの場合もそうであることは容易に想像がつくだろう。\n",
        "\n",
        "では、2つの分布間の余弦類似度を最大化しようとするとどうなるか、試してみましょう。\n"
      ],
      "metadata": {
        "id": "_FHKsYrXQjIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.linspace(-mu1-mu2-5*sigma1-5*sigma2,mu1+mu2+5*sigma1+5*sigma2,1000)\n",
        "px = gaussian1.log_prob(x).exp() + gaussian2.log_prob(x).exp()\n",
        "mu,sigma,all_qx = optimize_loss(px, loss_fn='cosine_similarity', muq = 5.0, sigmaq = 1.0,mode='max')\n",
        "\n",
        "fn = 'cosine_similarity.gif'\n",
        "ani = create_animation(x,px,all_qx,fn=fn)\n",
        "Image(f\"{fn}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "_LffkEN3QjIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 結論\n",
        "上記のように1次元の場合、最も近い平均値に収束します。複数の谷がある高次元空間では、MSE/L1 Lossを最小化すると異なる結果になる可能性がある。 ディープラーニングでは、ニューラルネットワークの重みをランダムに初期化する。そのため、同じニューラルネットワークの異なる実行において、異なる局所最小値に向かって収束するのは理にかなっている。\n",
        "確率的重み平均のようなテクニックは、異なる局所最小値への重みを提供するため、おそらく汎化性が向上する。異なる局所最小値がデータセットに関する重要な情報を内包している可能性がある。\n",
        "\n",
        "次回は、ワッサーシュタイン距離について考えてみたい。"
      ],
      "metadata": {
        "trusted": true,
        "id": "8sWD8Cq6QjII"
      }
    }
  ]
}