{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/ML/blob/main/OpenAI_Whisper_ASR_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web App Demonstrating OpenAI's Whisper Speech Recognition Model\n",
        "\n",
        "This is a Colab notebook that allows you to record or upload audio files to [OpenAI's free Whisper speech recognition model](https://openai.com/blog/whisper/). This was based on [an original notebook by @amrrs](https://github.com/amrrs/openai-whisper-webapp), with added documentation and test files by [Pete Warden](https://twitter.com/petewarden).\n",
        "\n",
        "To use it, choose `Runtime->Run All` from the Colab menu. If you're viewing this notebook on GitHub, follow [this link](https://colab.research.google.com/github/petewarden/openai-whisper-webapp/blob/main/OpenAI_Whisper_ASR_Demo.ipynb) to open it in Colab first. After about a minute or so, you should see a button at the bottom of the page with a `Record from microphone` link. Click this, you'll be asked to give permission to access your mic, and then speak for up to 30 seconds. Once you're done, press `Stop recording`, and a transcript of the first 30 seconds of your speech should soon appear in the box to the right of the recording button. To transcribe more speech, click `Clear' in the left box and start over.\n",
        "\n",
        "You can also upload your own audio samples using the folder icon on the left of this page. That gives you access to a file system you can upload to by dragging files into it. You can see examples of how to run the transcription in a couple of the cells below."
      ],
      "metadata": {
        "id": "Lbja1jB3vDOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the Whisper Code"
      ],
      "metadata": {
        "id": "kosakhNmxb7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsJUxc0aRsAf"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the ML Model"
      ],
      "metadata": {
        "id": "AtAvuKSJxhNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"small\") # tiny, base, samll, medium, large, turbo\n"
      ],
      "metadata": {
        "id": "Kr5faKybKi4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check we have a GPU\n",
        "\n",
        "You should see the output `device(type='cuda', index=0)` below. If you don't, you may be on a CPU-only Colab instance which will run more slowly. Go to `Runtime->Change Runtime Type` to fix this."
      ],
      "metadata": {
        "id": "e200RNNlxn5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "id": "u_6_s2iHboR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Test Audio Files\n",
        "\n",
        "This repository has a couple of pre-recorded MP3s to run through the transcribe function. You can listen to them with the audio widgets displayed below."
      ],
      "metadata": {
        "id": "VqbPlIrVx9sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/petewarden/openai-whisper-webapp"
      ],
      "metadata": {
        "id": "xOpHUlfqth1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio(\"/content/openai-whisper-webapp/mary.mp3\")"
      ],
      "metadata": {
        "id": "fhLths-Nfn5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio(\"/content/openai-whisper-webapp/daisy_HAL_9000.mp3\")"
      ],
      "metadata": {
        "id": "_8vKBR6cfxm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Transcribe Function\n",
        "\n",
        "Now we've loaded the model, and have the code, this is the function that takes an audio file path as an input and returns the recognized text (and logs what it thinks the language is)."
      ],
      "metadata": {
        "id": "QwLTZtubySoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe(audio):\n",
        "\n",
        "    # load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    # detect the spoken language\n",
        "    _, probs = model.detect_language(mel)\n",
        "    print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "\n",
        "    # decode the audio\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    return result.text\n"
      ],
      "metadata": {
        "id": "JtTvvQQPcOZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test with Pre-Recorded Audio\n",
        "\n",
        "Before we bring up the UI to allow you to record your own live audio, we're going to run the `transcribe()` function on a couple of MP3s we've downloaded. You should see `Mary had a little lamb, its fleece was white as snow, and everywhere that Mary went, the lamb was sure to go.` for `mary.mp3`, which I recorded as an example of clear audio. The second file is a lot harder to transcribe, with very distorted audio, but the model does a good job with `Tazy, Tazy, Tazy. Give me your answer to time after crazy all for the love of you. It won't be a stylish marriage`. You'll notice the transcript is cut off after 30 seconds, which is the default length for this notebook. It can be extended, but that's outside of the scope of this documentation."
      ],
      "metadata": {
        "id": "_QRwhuOXynZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "easy_text = transcribe(\"/content/openai-whisper-webapp/mary.mp3\")\n",
        "print(easy_text)\n",
        "\n",
        "hard_text = transcribe(\"/content/openai-whisper-webapp/daisy_HAL_9000.mp3\")\n",
        "print(hard_text)"
      ],
      "metadata": {
        "id": "bDXgLIprIsAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the Web UI Toolkit\n",
        "\n",
        "We'll be using gradio to provide the widgets we need to do audio recording."
      ],
      "metadata": {
        "id": "9ojRF2zWzcYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio==3.50"
      ],
      "metadata": {
        "id": "fjM27tWsI4dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time"
      ],
      "metadata": {
        "id": "ILFOYNnTcYe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Interface\n",
        "\n",
        "After running this script, you should see two widgets below that you can use to record live audio and see the transcription, as described in the introduction."
      ],
      "metadata": {
        "id": "2tHwfOG-zlY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gr.Interface(\n",
        "    title = 'OpenAI Whisper ASR Gradio Web UI',\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        \"textbox\"\n",
        "    ],\n",
        "    live=True).launch()"
      ],
      "metadata": {
        "id": "deSAVvfJcWBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2Zid2MKdPxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}