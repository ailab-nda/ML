{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/ML/blob/main/Optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer解説\n",
        "by CyberWei\n",
        "\n",
        "https://www.cyberwei.com/archives/230530jp"
      ],
      "metadata": {
        "id": "xiSc-owX9Zu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "import multiprocessing\n",
        "\n",
        "e = 2.718281828459045\n",
        "pi = 3.14159265359"
      ],
      "metadata": {
        "id": "FljyPM8r69uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn_1(para):\n",
        "    return 20 * (para[0] ** 2) + 2 * (para[1] ** 2)\n",
        "def loss_fn_2(para):\n",
        "    return 2 * (para[0] ** 2) + 10 * (para[1] ** 2) + 10 * (e**(para[1]*5j)).imag\n",
        "def loss_fn_3(para):\n",
        "    return 20 * (para[0] ** 2) + 2 * (para[1] ** 2) + (e**(para[1]*5j)).imag"
      ],
      "metadata": {
        "id": "bK-lg04L7Ai_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class show_plot():\n",
        "    def __init__(self):\n",
        "        a = np.arange(-2, 2, 0.01)\n",
        "        b = np.arange(-2, 2, 0.01)\n",
        "        self.X, self.Y = np.meshgrid(a, b)\n",
        "    def show(self, loss_fn):\n",
        "        plt.figure(figsize=(5,5))\n",
        "        Z = loss_fn([self.X,self.Y])\n",
        "        plt.contour(self.X, self.Y, Z, levels=20)\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.title(loss_fn.__name__)\n",
        "        plt.show()\n",
        "\n",
        "show = show_plot()\n",
        "\n",
        "show.show(loss_fn_1)\n",
        "show.show(loss_fn_2)\n",
        "show.show(loss_fn_3)"
      ],
      "metadata": {
        "id": "fCAeb5qL7CL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step_calculate(optimizer, loss_fn):\n",
        "    loss_previous = 0\n",
        "    for i in range(200):\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(*optimizer.param_groups[0]['params'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if abs(loss - loss_previous) < 0.01:\n",
        "            break;\n",
        "        loss_previous = loss\n",
        "    return i\n",
        "\n",
        "def route_calculate(optimizer, loss_fn):\n",
        "    history = []\n",
        "    for i in range(50):\n",
        "        optimizer.zero_grad()\n",
        "        history.append([*optimizer.param_groups[0]['params'][0].detach().tolist()])\n",
        "        loss = loss_fn(*optimizer.param_groups[0]['params'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return history"
      ],
      "metadata": {
        "id": "W6gTXfZ27Da2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 降下ルート"
      ],
      "metadata": {
        "id": "lKnuzMM88zef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = loss_fn_2\n",
        "lr = 0.2\n",
        "\n",
        "def route_with_different_location_SGD(p):\n",
        "    param = torch.tensor(p, requires_grad=True)\n",
        "    optimizer = optim.SGD([param], lr = 0.01)\n",
        "    return route_calculate(optimizer, loss_fn)\n",
        "\n",
        "def route_with_different_location_SGDm(p):\n",
        "    param = torch.tensor(p, requires_grad=True)\n",
        "    optimizer = optim.SGD([param], lr = 0.01, momentum = 0.5)\n",
        "    return route_calculate(optimizer, loss_fn)\n",
        "\n",
        "def route_with_different_location_Adagrad(p):\n",
        "    param = torch.tensor(p, requires_grad=True)\n",
        "    optimizer = optim.Adagrad([param], lr = lr)\n",
        "    return route_calculate(optimizer, loss_fn)\n",
        "\n",
        "def route_with_different_location_RMSprop(p):\n",
        "    param = torch.tensor(p, requires_grad=True)\n",
        "    optimizer = optim.RMSprop([param], lr = lr)\n",
        "    return route_calculate(optimizer, loss_fn)\n",
        "\n",
        "def route_with_different_location_Adadelta(p):\n",
        "    param = torch.tensor(p, requires_grad=True)\n",
        "    optimizer = optim.Adadelta([param], lr = 50.0)\n",
        "    return route_calculate(optimizer, loss_fn)\n",
        "\n",
        "def route_with_different_location_ADAM(p):\n",
        "    param = torch.tensor(p, requires_grad=True)\n",
        "    optimizer = optim.Adam([param], lr = lr)\n",
        "    return route_calculate(optimizer, loss_fn)\n",
        "\n",
        "def route_with_different_location_ADAMW(p):\n",
        "    param = torch.tensor(p, requires_grad=True)\n",
        "    optimizer = optim.AdamW([param], lr = lr)\n",
        "    return route_calculate(optimizer, loss_fn)\n",
        "\n",
        "\n",
        "def route_with_different_location_SGDs(p):\n",
        "    param = torch.tensor(p, requires_grad=True)\n",
        "    optimizer = optim.SGD([param], lr = 0.01)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "    history = []\n",
        "    for i in range(50):\n",
        "        optimizer.zero_grad()\n",
        "        history.append([*optimizer.param_groups[0]['params'][0].detach().tolist()])\n",
        "        loss = loss_fn(*optimizer.param_groups[0]['params'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        exp_lr_scheduler.step()\n",
        "    return history\n",
        "\n",
        "def route_with_different_location_SGDms(p):\n",
        "    param = torch.tensor(p, requires_grad=True)\n",
        "    optimizer = optim.SGD([param], lr = 0.01, momentum = 0.5)\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "    history = []\n",
        "    for i in range(50):\n",
        "        optimizer.zero_grad()\n",
        "        history.append([*optimizer.param_groups[0]['params'][0].detach().tolist()])\n",
        "        loss = loss_fn(*optimizer.param_groups[0]['params'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        exp_lr_scheduler.step()\n",
        "    return history\n",
        "\n",
        "route_with_different_locations = [route_with_different_location_SGD,\n",
        "                               route_with_different_location_SGDm,\n",
        "                               route_with_different_location_Adagrad,\n",
        "                               route_with_different_location_RMSprop,\n",
        "                               route_with_different_location_Adadelta,\n",
        "                               route_with_different_location_ADAM,\n",
        "                               route_with_different_location_ADAMW,\n",
        "                               route_with_different_location_SGDs,\n",
        "                               route_with_different_location_SGDms]\n",
        "\n",
        "x_start, y_start = np.arange(-2, 2.1, 0.9), np.arange(1.5, 2.1, 0.9)\n",
        "X_start, Y_start = np.meshgrid(x_start, y_start)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fig = plt.figure(figsize=(15, (int(len(route_with_different_locations) / 3) + 1) * 5), dpi = 200)\n",
        "    for index, route_with_different_location in enumerate(route_with_different_locations):\n",
        "        print(f'Calculating [{route_with_different_location.__name__}]...')\n",
        "\n",
        "        plt.subplot(int(len(route_with_different_locations)/3) + 1, 3, index + 1)\n",
        "        plt.title(route_with_different_locations[index].__name__)\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        a = np.arange(-2, 2, 0.01)\n",
        "        b = np.arange(-2, 2, 0.01)\n",
        "        X, Y = np.meshgrid(a, b)\n",
        "        Z = loss_fn([X,Y])\n",
        "        plt.contour(X, Y, Z, levels=20)\n",
        "\n",
        "        with Pool(4) as p:\n",
        "            routes = np.array(p.map(route_with_different_location, zip(X_start.flatten(),Y_start.flatten())))\n",
        "        for index, a in enumerate(routes):\n",
        "            route = routes[index]\n",
        "            plt.plot(route[:,0],route[:,1])\n",
        "            plt.scatter(route[:,0], route[:,1])\n",
        "\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "i9QPOJsG7L5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 学習率のオプティマイザへの影響\n",
        "(Takes some time to calculate)"
      ],
      "metadata": {
        "id": "N2QUiXjf7GWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = loss_fn_3\n",
        "\n",
        "def step_with_different_lr_SGD(lr):\n",
        "    param = torch.tensor([x,y], requires_grad=True)\n",
        "    optimizer = optim.SGD([param], lr = lr)\n",
        "    return lr, step_calculate(optimizer, loss_fn)\n",
        "\n",
        "def step_with_different_lr_SGDm(lr):\n",
        "    param = torch.tensor([x,y], requires_grad=True)\n",
        "    optimizer = optim.SGD([param], lr = lr, momentum = 0.5)\n",
        "    return lr, step_calculate(optimizer, loss_fn)\n",
        "\n",
        "def step_with_different_lr_Adagrad(lr):\n",
        "    param = torch.tensor([x,y], requires_grad=True)\n",
        "    optimizer = optim.Adagrad([param], lr = lr)\n",
        "    return lr, step_calculate(optimizer, loss_fn)\n",
        "\n",
        "def step_with_different_lr_RMSprop(lr):\n",
        "    param = torch.tensor([x,y], requires_grad=True)\n",
        "    optimizer = optim.RMSprop([param], lr = lr)\n",
        "    return lr, step_calculate(optimizer, loss_fn)\n",
        "\n",
        "def step_with_different_lr_Adadelta(lr):\n",
        "    param = torch.tensor([x,y], requires_grad=True)\n",
        "    optimizer = optim.Adadelta([param], lr = lr)\n",
        "    return lr, step_calculate(optimizer, loss_fn)\n",
        "\n",
        "def step_with_different_lr_ADAM(lr):\n",
        "    param = torch.tensor([x,y], requires_grad=True)\n",
        "    optimizer = optim.Adam([param], lr = lr)\n",
        "    return lr, step_calculate(optimizer, loss_fn)\n",
        "\n",
        "def step_with_different_lr_ADAMW(lr):\n",
        "    param = torch.tensor([x,y], requires_grad=True)\n",
        "    optimizer = optim.AdamW([param], lr = lr)\n",
        "    return lr, step_calculate(optimizer, loss_fn)\n",
        "\n",
        "\n",
        "\n",
        "step_with_different_lrs = [step_with_different_lr_SGD,\n",
        "                           step_with_different_lr_SGDm,\n",
        "                           step_with_different_lr_Adagrad,\n",
        "                           step_with_different_lr_RMSprop,\n",
        "                           #step_with_different_lr_Adadelta,\n",
        "                           step_with_different_lr_ADAM,\n",
        "                           step_with_different_lr_ADAMW]\n",
        "\n",
        "x, y = 1.0, 2.0 #Starting point\n",
        "data = np.arange(0, 1, 0.01) #Range of Learning Rate\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fig = plt.figure(figsize=(15, (int(len(step_with_different_lrs) / 3) + 1) * 5), dpi = 200)\n",
        "    for index, step_with_different_lr in enumerate(step_with_different_lrs):\n",
        "        print(f'Calculating [{step_with_different_lrs[index].__name__}]...')\n",
        "        with Pool(4) as p:\n",
        "            step = np.array(p.map(step_with_different_lr, data))\n",
        "        print('Finish!')\n",
        "        plt.subplot(int(len(step_with_different_lrs)/3) + 1, 3, index + 1)\n",
        "        plt.plot(step[:,0],step[:,1])\n",
        "        plt.xlabel('lr')\n",
        "        plt.ylabel('steps')\n",
        "        plt.ylim((0,210))\n",
        "        plt.title(step_with_different_lrs[index].__name__)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oespkCkm7FbK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}