{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/NLP/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1zY4P3XUM2e"
      },
      "source": [
        "# Transformer による自然言語処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KW1W5AOUowz"
      },
      "source": [
        "### 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvlszIhar1Dk"
      },
      "source": [
        "関連ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h5Y7k8e3WQy"
      },
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTr0TD-9r4OL"
      },
      "source": [
        "関連ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFLORpjArCGT"
      },
      "source": [
        "import torch\n",
        "import textwrap\n",
        "from transformers import T5Tokenizer, BertTokenizer\n",
        "from transformers import AutoModelForCausalLM, RobertaForMaskedLM\n",
        "from transformers import BertForPreTraining"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snol5wIUawe"
      },
      "source": [
        "## 1. 日本語モデルによる文章中の空欄埋め\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ta1Zf-gUxJA"
      },
      "source": [
        "### モデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDMZTkNRRvkl"
      },
      "source": [
        "# トーカナイザの設定\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-roberta-base\")\n",
        "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
        "# モデルの設定\n",
        "model = RobertaForMaskedLM.from_pretrained(\"rinna/japanese-roberta-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYo4P-RpU1g3"
      },
      "source": [
        "### 問題文の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_y3KX3R9Fo"
      },
      "source": [
        "# 原文\n",
        "text = \"4年に1度オリンピックは開かれる。\"\n",
        "\n",
        "# 文頭に [CLS] を付加\n",
        "text = \"[CLS]\" + text + \"[SEP]\"\n",
        "\n",
        "# トークン化\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# トークンにマスクをかける\n",
        "masked_idx = 5\n",
        "tokens[masked_idx] = tokenizer.mask_token\n",
        "print(tokens) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lao7fOXYU_df"
      },
      "source": [
        "### 穴埋め問題を解く"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XHqe2k4WPt8"
      },
      "source": [
        "補充すべき単語の推定 (id)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfCfwo7vSOmb"
      },
      "source": [
        "# トークンから単語 ID に変換\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(token_ids)\n",
        "\n",
        "# テンソルに変換\n",
        "token_tensor = torch.LongTensor([token_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s6nSS5YWZhy"
      },
      "source": [
        "結果の表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBBgnyLhSgpf"
      },
      "source": [
        "# 場所の ID を与える\n",
        "position_ids = list(range(0, token_tensor.size(1)))\n",
        "position_id_tensor = torch.LongTensor([position_ids])\n",
        "\n",
        "# マスクされたトークンの予測値（トップ 10）\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=token_tensor, position_ids=position_id_tensor)\n",
        "    predictions = outputs[0][0, masked_idx].topk(10)\n",
        "\n",
        "for i, index_t in enumerate(predictions.indices):\n",
        "    index = index_t.item()\n",
        "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
        "    print(i, token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr9EfcyC_iWM"
      },
      "source": [
        "## 2. 英語モデルによる TOEIC Part 5 の解答"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58-TMKep_qJs"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1eiAYAk_2pG"
      },
      "source": [
        "### 問題文の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSmSRnUC_yaL"
      },
      "source": [
        "text = \"Customer reviews indicate that many modern mobile devices are often unnecessarily [MASK] .\"\n",
        "candidate = [\"complication\", \"complicates\", \"complicate\", \"complicated\"]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSTAJXehHH0f"
      },
      "source": [
        "BERTに分かるように変換 (text --> tokens)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtd6zVLVAFWO"
      },
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "masked_index = tokens.index(\"[MASK]\")\n",
        "tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "\n",
        "print(tokens)\n",
        "print(masked_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxCqQlnVAkB1"
      },
      "source": [
        "### 解答の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br8Q8xIAAl9d"
      },
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "ids = torch.tensor(ids).reshape(1,-1)  # バッチサイズ1の形に整形\n",
        "predictions = model(ids)[0][0]\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJAOZ3FsAyuO"
      },
      "source": [
        "_, predicted_indexes = torch.topk(predictions[masked_index+1], k=1000)\n",
        "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes.tolist())\n",
        "# -> ['expensive', 'small', 'priced', 'used', ...\n",
        "print(predicted_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74CckPLHDmrT"
      },
      "source": [
        "for i, v in enumerate(predicted_tokens):\n",
        "    if v in candidate:\n",
        "        print(i, v)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO7K_6PyOZ6x"
      },
      "source": [
        "### 解答の作成を関数にする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kny1iT_eED6I"
      },
      "source": [
        "def part5_slover(text, candidate):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    masked_index = tokens.index(\"[MASK]\")\n",
        "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    ids = torch.tensor(ids).reshape(1,-1)\n",
        "    predictions = model(ids)[0][0]\n",
        "\n",
        "    _, predicted_indexes = torch.topk(predictions[masked_index+1], k=10000)\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes.tolist())\n",
        "\n",
        "    for i, v in enumerate(predicted_tokens):\n",
        "        if v in candidate:\n",
        "            return \"answer: \" + v\n",
        "    return \"don't know\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Hercu2WqfS"
      },
      "source": [
        "### 練習問題で試してみる\n",
        "公式サンプル問題 --> https://www.iibc-global.org/toeic/test/lr/about/format/sample05.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucJl0dG5JgG0"
      },
      "source": [
        "text2 = \"Jamal Nawzad has received top performance reviews [MASK] he joined the sales department two years ago .\"\n",
        "candidate2 = [\"despite\", \"except\", \"since\", \"during\"]\n",
        "text3 = \"Gyeon Corporation’s continuing education policy states that [MASK] learning new skills enhances creativity and focus .\"\n",
        "candidate3 = [\"regular\", \"regularity\", \"regulate\", \"regularly\"]\n",
        "text4 = \"Among [MASK] recognized at the company awards ceremony were senior business analyst Natalie Obi and sales associate Peter Comeau. .\"\n",
        "candidate4 = [\"who\", \"whose\", \"they\", \"those\"]\n",
        "text5 = \"All clothing sold in Develyn’s Boutique is made from natural materials and contains no [MASK] dyes .\"\n",
        "candidate5 = [\"immediate\", \"synthetic\", \"reasonable\", \"assumed\"]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q_Vi027EGFL"
      },
      "source": [
        "part5_slover(text2, candidate2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pla_3YxVN_g"
      },
      "source": [
        "## 3. GPT-2 による文書生成\n",
        "GTP-2 の日本語モデルは複数あります。ここでは２つ紹介しますが、どちらを使ってもいいです（用意するデータ形式がちょっと違うので注意）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz2_A1zdkE3X"
      },
      "source": [
        "### (1) rinna/japanese-gpt2 の利用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THovObjOV18G"
      },
      "source": [
        "### モデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYr-6Sk4RXBb"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoa-OUqiWECt"
      },
      "source": [
        "### 文書生成の例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuj3d3wOE_cJ"
      },
      "source": [
        "input = tokenizer.encode(\"私が防衛大学校を志望したのは、\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=200, num_return_sequences=8)\n",
        "sentences = tokenizer.batch_decode(output)\n",
        "print(\"=========================================================\")\n",
        "for i in sentences:\n",
        "    print(textwrap.fill(i.replace('</s> ', ''), 50))\n",
        "    print(\"=========================================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzH5l0XrEkJq"
      },
      "source": [
        "### 文体の変更（モデルのチューニング）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJNI3s4Dk0l_"
      },
      "source": [
        "ここで、学内メールに添付されているされている train.txt と run_clm.py のアップロードを行う（左のファイルウィンドウにドラッグ＆ドロップ）。\n",
        "\n",
        "train.txt は自分で用意したものも試してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEotsURZgNfq"
      },
      "source": [
        "%%time\n",
        "!rm -r output\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-small \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=30 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --output_dir=output/ \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXZQf15Sgeik"
      },
      "source": [
        "# モデルの準備\n",
        "model2 = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "\n",
        "# 推論\n",
        "input = tokenizer.encode(\"私が防衛大学校を志望したのは、\", return_tensors=\"pt\")\n",
        "output = model2.generate(input, do_sample=True, max_length=200, num_return_sequences=8)\n",
        "sentences = tokenizer.batch_decode(output)\n",
        "print(\"=========================================================\")\n",
        "for i in sentences:\n",
        "    print(textwrap.fill(i.replace('</s>', ''), 50))\n",
        "    print(\"=========================================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfnrGkVoj-5j"
      },
      "source": [
        "### (2) GPT2-Japanese の利用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1PcyX7Qku14"
      },
      "source": [
        "### モデルのダウンロードとインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvM-z3pdOZEi"
      },
      "source": [
        "# gpt2-japaneseのインストール\n",
        "!git clone https://github.com/tanreinama/gpt2-japanese\n",
        "%cd gpt2-japanese\n",
        "#!pip uninstall tensorflow -y\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC2ExWNabCeH"
      },
      "source": [
        "# smallモデルのダウンロード\n",
        "!wget https://www.nama.ne.jp/models/gpt2ja-small.tar.bz2\n",
        "!tar xvfj gpt2ja-small.tar.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAcoVLUgk5Rt"
      },
      "source": [
        "### ランダムな文章の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDL1l6hdUNX"
      },
      "source": [
        "# smallモデルの動作確認\n",
        "!python gpt2-generate.py --model gpt2ja-small --num_generate 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lp-WPCTk1ph"
      },
      "source": [
        "### 文章の続きを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYG-XCyHebGa"
      },
      "source": [
        "!python gpt2-generate.py --model gpt2ja-small --num_generate 5 --context=\"私が防衛大学校を志望したのは、\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1gDAqQOEdIS"
      },
      "source": [
        "### 文体の変更（モデルのチューニング）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaUhdN9recTk"
      },
      "source": [
        "# データセットの作成\n",
        "!git clone https://github.com/tanreinama/Japanese-BPEEncoder.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hg24DNNm0e4"
      },
      "source": [
        "ここで、gtp2-japanese の下に mydata というフォルダを作成し、データを置く。データ内の文章の終わりに <|endoftext|> タグを付ける。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuxnmlayoG-g"
      },
      "source": [
        "!python ./Japanese-BPEEncoder/encode_bpe.py --src_dir mydata --dst_file finetune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqCmn2uYwJ6K"
      },
      "source": [
        "ここで、run_finetune.py を gtp2-japanese の下に置く。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAwSANKXoPtU"
      },
      "source": [
        "!python run_finetune.py --base_model gpt2ja-small --dataset finetune.npz --run_name gpr2ja-finetune_run1-small"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL2paV3RppP2"
      },
      "source": [
        "!python gpt2-generate.py --model checkpoint/gpr2ja-finetune_run1-small --num_generate 5 --context=\"私が防衛大学校を志望したのは、\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}