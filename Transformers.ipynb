{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb のコピー",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/ML/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1zY4P3XUM2e"
      },
      "source": [
        "# Transformer による自然言語処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KW1W5AOUowz"
      },
      "source": [
        "### 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvlszIhar1Dk"
      },
      "source": [
        "関連ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h5Y7k8e3WQy"
      },
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTr0TD-9r4OL"
      },
      "source": [
        "関連ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFLORpjArCGT"
      },
      "source": [
        "import torch\n",
        "import textwrap\n",
        "from transformers import T5Tokenizer, BertTokenizer\n",
        "from transformers import AutoModelForCausalLM, RobertaForMaskedLM\n",
        "from transformers import BertForPreTraining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snol5wIUawe"
      },
      "source": [
        "## 1. 日本語モデルによる文章中の空欄埋め\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ta1Zf-gUxJA"
      },
      "source": [
        "### モデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDMZTkNRRvkl"
      },
      "source": [
        "# トーカナイザの設定\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-roberta-base\")\n",
        "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
        "# モデルの設定\n",
        "model = RobertaForMaskedLM.from_pretrained(\"rinna/japanese-roberta-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYo4P-RpU1g3"
      },
      "source": [
        "### 問題文の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_y3KX3R9Fo"
      },
      "source": [
        "# 原文\n",
        "text = \"4年に1度オリンピックは開かれる。\"\n",
        "\n",
        "# 文頭に [CLS] を付加\n",
        "text = \"[CLS]\" + text + \"[SEP]\"\n",
        "\n",
        "# トークン化\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# トークンにマスクをかける\n",
        "masked_idx = 5\n",
        "tokens[masked_idx] = tokenizer.mask_token\n",
        "print(tokens) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lao7fOXYU_df"
      },
      "source": [
        "### 穴埋め問題を解く"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XHqe2k4WPt8"
      },
      "source": [
        "補充すべき単語の推定 (id)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfCfwo7vSOmb"
      },
      "source": [
        "# トークンから単語 ID に変換\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(token_ids)\n",
        "\n",
        "# テンソルに変換\n",
        "token_tensor = torch.LongTensor([token_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s6nSS5YWZhy"
      },
      "source": [
        "結果の表示\n",
        "\n",
        "空欄に当てはまると思われる単語を確信の高い順に複数出力します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBBgnyLhSgpf"
      },
      "source": [
        "# 場所の ID を与える\n",
        "position_ids = list(range(0, token_tensor.size(1)))\n",
        "position_id_tensor = torch.LongTensor([position_ids])\n",
        "\n",
        "# マスクされたトークンの予測値（トップ 10）\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=token_tensor, position_ids=position_id_tensor)\n",
        "    predictions = outputs[0][0, masked_idx].topk(10)\n",
        "\n",
        "for i, index_t in enumerate(predictions.indices):\n",
        "    index = index_t.item()\n",
        "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
        "    print(i, token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "コンピュータが出してきた結果が妥当であるか、確認してください。"
      ],
      "metadata": {
        "id": "HLaSFHs9B_Il"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr9EfcyC_iWM"
      },
      "source": [
        "## 2. 英語モデルによる TOEIC Part 5 の解答"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の穴埋めを英語の文章でやってみます。"
      ],
      "metadata": {
        "id": "3RWP2rCGCJTX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58-TMKep_qJs"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1eiAYAk_2pG"
      },
      "source": [
        "### 問題文の作成"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "問題文：text、選択肢：candidate"
      ],
      "metadata": {
        "id": "sN_PE0B6gY52"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSmSRnUC_yaL"
      },
      "source": [
        "text = \"Customer reviews indicate that many modern mobile devices are often unnecessarily [MASK] .\"\n",
        "candidate = [\"complication\", \"complicates\", \"complicate\", \"complicated\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSTAJXehHH0f"
      },
      "source": [
        "BERTに分かるように変換 (text --> tokens)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtd6zVLVAFWO"
      },
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "masked_index = tokens.index(\"[MASK]\")\n",
        "tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "\n",
        "print(tokens)\n",
        "print(masked_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxCqQlnVAkB1"
      },
      "source": [
        "### 解答の作成"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "日本語のように穴埋めでやったように候補をたくさん出力し、選択肢と一致する最初の単語を問題の答とします。"
      ],
      "metadata": {
        "id": "96I-9j_FCaTu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br8Q8xIAAl9d"
      },
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "ids = torch.tensor(ids).reshape(1,-1)  # バッチサイズ1の形に整形\n",
        "predictions = model(ids)[0][0]\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJAOZ3FsAyuO"
      },
      "source": [
        "_, predicted_indexes = torch.topk(predictions[masked_index+1], k=1000)\n",
        "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes.tolist())\n",
        "# -> ['expensive', 'small', 'priced', 'used', ...\n",
        "print(predicted_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74CckPLHDmrT"
      },
      "source": [
        "for i, v in enumerate(predicted_tokens):\n",
        "    if v in candidate:\n",
        "        print(i, v)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "得られた答えが自分で考えた答えと一致したか確認してください。"
      ],
      "metadata": {
        "id": "fpZYfkpXCSS8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO7K_6PyOZ6x"
      },
      "source": [
        "### 解答の作成を関数にする"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまで一連の流れを関数にして、簡単に実行できるようにします。"
      ],
      "metadata": {
        "id": "JZYDWm2gDJZ4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kny1iT_eED6I"
      },
      "source": [
        "def part5_slover(text, candidate):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    masked_index = tokens.index(\"[MASK]\")\n",
        "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    ids = torch.tensor(ids).reshape(1,-1)\n",
        "    predictions = model(ids)[0][0]\n",
        "\n",
        "    _, predicted_indexes = torch.topk(predictions[masked_index+1], k=10000)\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes.tolist())\n",
        "\n",
        "    for i, v in enumerate(predicted_tokens):\n",
        "        if v in candidate:\n",
        "            return \"answer: \" + v\n",
        "    return \"don't know\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Hercu2WqfS"
      },
      "source": [
        "### 練習問題で試してみる\n",
        "上記は、公式サンプル問題 --> https://www.iibc-global.org/toeic/test/lr/about/format/sample05.html　からの出題でした。残りが４問あるので、まずは自分で答えてみてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では、残りの問題をコンピュータに答えさせます。"
      ],
      "metadata": {
        "id": "qjWl_sxQDsum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "第２問〜第５問（問題文：text2〜5、選択肢：candidate2〜5）の入力"
      ],
      "metadata": {
        "id": "XVx135_CfxRO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucJl0dG5JgG0"
      },
      "source": [
        "text2 = \"Jamal Nawzad has received top performance reviews [MASK] he joined the sales department two years ago .\"\n",
        "candidate2 = [\"despite\", \"except\", \"since\", \"during\"]\n",
        "text3 = \"Gyeon Corporation’s continuing education policy states that [MASK] learning new skills enhances creativity and focus .\"\n",
        "candidate3 = [\"regular\", \"regularity\", \"regulate\", \"regularly\"]\n",
        "text4 = \"Among [MASK] recognized at the company awards ceremony were senior business analyst Natalie Obi and sales associate Peter Comeau. .\"\n",
        "candidate4 = [\"who\", \"whose\", \"they\", \"those\"]\n",
        "text5 = \"All clothing sold in Develyn’s Boutique is made from natural materials and contains no [MASK] dyes .\"\n",
        "candidate5 = [\"immediate\", \"synthetic\", \"reasonable\", \"assumed\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "解答の出力"
      ],
      "metadata": {
        "id": "JI6h-_lyf_Pi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q_Vi027EGFL"
      },
      "source": [
        "print(part5_slover(text2, candidate2))\n",
        "print(part5_slover(text3, candidate3))\n",
        "print(part5_slover(text4, candidate4))\n",
        "print(part5_slover(text5, candidate5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pla_3YxVN_g"
      },
      "source": [
        "## 3. GPT-2 による文書生成\n",
        "GTP-2 の日本語モデルは複数あります。本実験では rinna のデータを用いたものを使います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz2_A1zdkE3X"
      },
      "source": [
        "### (1) rinna/japanese-gpt2 の利用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THovObjOV18G"
      },
      "source": [
        "### モデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYr-6Sk4RXBb"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoa-OUqiWECt"
      },
      "source": [
        "### 文書生成の例"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "防大の志望動機を書かせてみます。"
      ],
      "metadata": {
        "id": "Swym8XRSEH8K"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuj3d3wOE_cJ"
      },
      "source": [
        "input = tokenizer.encode(\"私が防衛大学校を志望したのは、\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=200, num_return_sequences=8)\n",
        "sentences = tokenizer.batch_decode(output)\n",
        "print(\"=========================================================\")\n",
        "for i in sentences:\n",
        "    print(textwrap.fill(i.replace('</s> ', ''), 50))\n",
        "    print(\"=========================================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzH5l0XrEkJq"
      },
      "source": [
        "### 文体の変更（モデルのチューニング）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJNI3s4Dk0l_"
      },
      "source": [
        "人工知能が出力する文章は、学習に使ったデータ次第で変わります。\n",
        "\n",
        "ここで、学内メールに添付されているされている train.txt と run_clm.py をアップロードしてください（画面左のファイルアイコンを押すとファイル画面がでますので、そこにドラッグ＆ドロップでアップロードできます）。\n",
        "\n",
        "アップロードができたら下の命令を実行してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEotsURZgNfq"
      },
      "source": [
        "%%time\n",
        "!rm -r output\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-small \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=30 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --output_dir=output/ \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "同じ書き出しで文章を生成します。"
      ],
      "metadata": {
        "id": "AofnnueREvSx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXZQf15Sgeik"
      },
      "source": [
        "# モデルの準備\n",
        "model2 = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "\n",
        "# 推論\n",
        "input = tokenizer.encode(\"私が防衛大学校を志望したのは、\", return_tensors=\"pt\")\n",
        "output = model2.generate(input, do_sample=True, max_length=200, num_return_sequences=8)\n",
        "sentences = tokenizer.batch_decode(output)\n",
        "print(\"=========================================================\")\n",
        "for i in sentences:\n",
        "    print(textwrap.fill(i.replace('</s>', ''), 50))\n",
        "    print(\"=========================================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "文体が変わったかどうかを確認してください。"
      ],
      "metadata": {
        "id": "mOH__sU-Ey6F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfnrGkVoj-5j"
      },
      "source": [
        "### 課題：自分のデータでモデルのチューニング\n",
        "\n",
        "train.txt に自分で用意した文章データを保存し、再度アップロードしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "新データで学習（チューニング）をします。"
      ],
      "metadata": {
        "id": "uWb8fj6TFhLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!rm -r output\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-small \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=30 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --output_dir=output/ \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "metadata": {
        "id": "noCJqa91FP_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "文章の生成：書き出しが「XXXX」となっているので自分のデータに合わせた書き出しにして実行してください。"
      ],
      "metadata": {
        "id": "tM3Uv00BFbkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの準備\n",
        "model2 = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "\n",
        "# 推論\n",
        "input = tokenizer.encode(\"XXXX、\", return_tensors=\"pt\")\n",
        "output = model2.generate(input, do_sample=True, max_length=200, num_return_sequences=8)\n",
        "sentences = tokenizer.batch_decode(output)\n",
        "print(\"=========================================================\")\n",
        "for i in sentences:\n",
        "    print(textwrap.fill(i.replace('</s>', ''), 50))\n",
        "    print(\"=========================================================\")"
      ],
      "metadata": {
        "id": "cXTznHWjFVol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "どのような文章が生成されたか、またチューニングがうまく行ったか、行かなかった場合にはどのような原因が考えられるか、などについて考察してください。"
      ],
      "metadata": {
        "id": "YLFR1RVRFrHf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YmjUeiytF23U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}