{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailab-nda/NLP/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1zY4P3XUM2e"
      },
      "source": [
        "# BERT による自然言語処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KW1W5AOUowz"
      },
      "source": [
        "### 準備（関連ライブラリのインストール）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h5Y7k8e3WQy"
      },
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q datasets"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr9EfcyC_iWM"
      },
      "source": [
        "## 学習済みモデルによる TOEIC Part 5 の解答"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58-TMKep_qJs",
        "outputId": "2abb3c97-2c51-4a28-a45e-94dced3cbf2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForPreTraining\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1eiAYAk_2pG"
      },
      "source": [
        "### 問題文の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSmSRnUC_yaL"
      },
      "source": [
        "text = \"Customer reviews indicate that many modern mobile devices are often unnecessarily [MASK] .\"\n",
        "candidate = [\"complication\", \"complicates\", \"complicate\", \"complicated\"]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSTAJXehHH0f"
      },
      "source": [
        "BERTに分かるように変換 (text --> tokens)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtd6zVLVAFWO",
        "outputId": "ce1d0223-12ca-4051-ea8a-193a4956b0f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "masked_index = tokens.index(\"[MASK]\")\n",
        "tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "\n",
        "print(tokens)\n",
        "print(masked_index)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['customer', 'reviews', 'indicate', 'that', 'many', 'modern', 'mobile', 'devices', 'are', 'often', 'un', '##ne', '##ces', '##sari', '##ly', '[MASK]', '.']\n",
            "['[CLS]', 'customer', 'reviews', 'indicate', 'that', 'many', 'modern', 'mobile', 'devices', 'are', 'often', 'un', '##ne', '##ces', '##sari', '##ly', '[MASK]', '.', '[SEP]']\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxCqQlnVAkB1"
      },
      "source": [
        "## 解答の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br8Q8xIAAl9d",
        "outputId": "f0b42a44-a979-443b-d289-b560f9577cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "ids = torch.tensor(ids).reshape(1,-1)  # バッチサイズ1の形に整形\n",
        "predictions = model(ids)[0][0]\n",
        "print(predictions)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -6.6169,  -6.6184,  -6.5846,  ...,  -5.8624,  -5.6686,  -3.7055],\n",
            "        [ -9.9890,  -9.9992, -10.0754,  ...,  -9.5736,  -8.6640,  -7.1756],\n",
            "        [ -1.2393,  -1.3488,  -1.7734,  ...,  -1.9904,  -3.6062,  -1.6186],\n",
            "        ...,\n",
            "        [ -0.8096,  -1.0072,  -0.7167,  ...,   0.0812,  -1.4079,   0.1565],\n",
            "        [-12.0386, -11.9543, -12.2466,  ...,  -9.7020, -10.8202,  -7.3316],\n",
            "        [ -9.9204, -10.3186, -10.5684,  ...,  -9.2968,  -9.4447,  -7.2326]],\n",
            "       grad_fn=<SelectBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJAOZ3FsAyuO",
        "outputId": "b2a057c1-41cd-4346-9fcb-2cf7935d8c1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "_, predicted_indexes = torch.topk(predictions[masked_index+1], k=1000)\n",
        "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes.tolist())\n",
        "# -> ['expensive', 'small', 'priced', 'used', ...\n",
        "print(predicted_tokens)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['expensive', 'small', 'priced', 'used', 'unreliable', 'cheap', 'noisy', 'mobile', 'portable', 'slow', 'costly', 'bulky', 'worn', 'outdated', 'poor', 'modified', 'large', 'upgraded', 'designed', 'damaged', 'popular', 'installed', 'available', 'inexpensive', 'robust', 'fragile', 'rugged', 'difficult', 'rare', 'useful', 'sophisticated', 'long', 'reliable', 'successful', 'new', 'produced', 'oversized', 'lightweight', 'defective', 'dull', 'problematic', 'heavy', 'limited', 'fast', 'unnecessary', 'late', 'redesigned', 'durable', 'old', 'bundled', 'updated', 'disabled', 'obsolete', 'purchased', 'competitive', 'functional', 'loaded', 'powered', 'replaced', 'short', 'packaged', 'ineffective', 'equipped', 'weak', 'modern', 'useless', 'customized', 'clumsy', 'unstable', 'sensitive', 'sized', 'responsive', 'ordered', 'low', 'complicated', 'annoying', 'recommended', 'disconnected', 'ignored', 'shipped', 'faulty', 'profitable', 'scarce', 'complex', 'dangerous', 'sold', 'busy', 'inaccurate', 'dated', 'handheld', 'made', 'hard', 'simple', 'brittle', 'vulnerable', 'flexible', 'unpopular', 'built', 'comfortable', 'affordable', 'loud', 'accessible', 'discontinued', 'accurate', 'compliant', 'automated', 'powerful', 'compatible', 'confused', 'secure', 'targeted', 'hot', 'uncomfortable', 'specialized', 'thin', 'unavailable', 'flat', 'affected', 'soft', 'modular', 'downloaded', 'early', 'manufactured', 'incompatible', 'inferior', 'deployed', 'neglected', 'high', 'confusing', 'unusual', 'flawed', 'delayed', 'removed', 'lacking', 'inadequate', 'inaccessible', 'charged', 'different', 'serviced', 'distributed', 'clean', 'abused', 'ugly', 'altered', 'predictable', 'cramped', 'selected', 'similar', 'young', 'aggressive', 'bought', 'fitted', 'electronic', 'desirable', 'curved', 'recalled', 'repetitive', 'disappointing', 'packed', 'changed', 'awkward', 'valuable', 'configured', 'constructed', 'lazy', 'present', 'boring', 'carried', 'dense', 'erratic', 'playable', 'compact', 'crude', 'efficient', 'programmed', 'padded', 'handy', 'stable', 'silent', 'idle', 'unfamiliar', 'purchases', 'cheaper', 'invasive', 'cold', 'easy', 'uncommon', 'ill', 'withdrawn', 'risky', 'unique', 'convenient', 'unsuccessful', 'versatile', 'attractive', 'gamer', 'improved', 'wireless', 'active', 'light', 'advanced', 'plagued', 'bandwidth', 'helpful', 'advertised', 'included', 'eccentric', 'sharp', 'disguised', 'niche', 'unsafe', 'smart', 'marketed', 'plain', 'imported', 'introduced', 'tested', 'stretched', 'lonely', 'strong', '##phobic', 'redundant', 'good', 'shaped', 'primitive', 'common', 'interactive', 'destructive', 'wired', 'criticized', 'unwanted', 'derivative', 'broken', 'quiet', 'utilized', 'tired', 'smaller', 'delivered', 'varied', 'important', 'unauthorized', 'placed', 'jammed', 'engineered', '##hone', 'supported', 'wasted', 'vain', 'bugs', 'unused', 'users', 'luxurious', 'tall', 'required', 'static', 'close', 'simplified', 'developed', 'launched', 'decorated', 'innovative', 'tiny', 'corrupt', 'unsuitable', 'named', 'adapted', 'released', 'tight', 'inappropriate', 'sleek', 'restricted', 'compromised', 'bright', 'current', 'volatile', 'smooth', 'friendly', 'personal', 'tuned', 'twisted', 'irritating', 'full', 'unpredictable', 'chargers', 'quick', 'bland', 'frustrating', 'generic', 'refurbished', 'unlocked', 'isolated', 'enhanced', 'wheeled', 'reduced', 'dilapidated', 'maintained', 'locked', 'harsh', 'handled', 'rejected', 'toxic', 'sweet', 'diverse', 'dark', 'broadband', 'messy', 'motorola', 'spartan', 'enabled', 'cool', 'imperfect', 'labeled', 'practical', 'effective', 'called', 'inflated', 'smartphone', 'unexpected', 'hacked', 'dry', 'motorized', 'widespread', 'chosen', 'unlikely', 'sparse', 'refined', 'timely', 'employed', 'familiar', 'delicate', 'slower', 'ubiquitous', '##kley', 'few', 'restrictive', 'stale', 'discarded', 'lucky', 'sterile', 'wrinkled', 'detailed', 'driven', 'nicknamed', 'intelligent', 'bored', 'stupid', 'free', 'extended', 'quickly', 'remote', 'commonplace', 'demanding', 'hooked', 'rated', 'fixed', 'disappointed', 'basic', 'rich', 'reluctant', 'armored', 'abandoned', 'extravagant', 'hazardous', 'wet', 'conservative', 'special', 'supplied', 'influential', 'variable', 'sturdy', 'frail', 'created', 'connected', 'absent', 'numerous', 'switched', 'cancelled', 'repaired', 'aging', 'bad', 'screwed', 'android', 'controversial', 'flop', 'tame', 'lame', 'needed', 'specific', 'empty', 'hollow', 'cramer', 'fit', 'inconsistent', 'newer', 'lost', 'downloadable', 'thirsty', 'faster', 'rushed', 'consumed', 'lax', 'worthless', 'regulated', 'cost', 'ornate', 'narrow', 'stuck', 'reused', 'stocked', 'mounted', '##charged', 'inserted', 'consumer', 'older', 'archaic', 'inactive', 'abusive', 'dim', 'incomplete', 'sexy', 'user', 'rebranded', 'complete', 'harmful', 'necessary', 'wealthy', 'massive', 'acquired', 'tapped', 'operated', 'naive', 'speedy', 'safe', 'worked', 'electronics', 'software', 'depressed', 'mechanical', 'warm', '##dget', 'polished', 'found', 'assembled', 'folded', 'dumb', 'immature', 'accessed', 'suitable', 'shiny', 'install', 'optimistic', 'stiff', 'modernized', 'beautiful', 'usable', 'activated', 'pressed', 'technical', 'adjusted', 'crowded', 'kerman', 'transparent', 'running', 'attached', 'filled', 'creative', 'commercial', 'classified', 'rigid', 'superior', 'ridden', 'moved', 'stacked', 'ready', 'cautious', 'homemade', 'rough', 'aged', 'failed', 'missed', 'upgrade', 'prevalent', 'integrated', 'stylized', 'bose', 'dedicated', 'battered', 'scratched', 'specialised', 'lengthy', 'run', 'interesting', 'floppy', 'stolen', 'careless', 'overlooked', 'unable', '##market', 'cracked', 'rusty', 'promoted', 'dusty', 'premature', 'fine', 'proprietary', 'eroded', 'intuitive', 'desired', 'upgrades', 'converted', 'valued', 'substituted', 'economical', 'avoided', 'discharged', 'disrupted', 'green', 'impacted', 'weighed', 'critical', 'modifying', 'compressed', 'expanded', 'samsung', 'parked', '##grade', 'technological', 'luxury', 'frequent', 'shipments', 'strapped', 'competing', 'strange', 'resistant', 'moody', 'functionality', 'crafted', '##icient', 'lean', 'changing', 'excessive', 'closed', 'tailored', 'experimental', 'replacing', 'simpler', 'accepted', 'requested', 'price', 'adopted', 'capable', 'complied', 'structured', 'distorted', 'impressive', 'poorly', 'embedded', 'hampered', 'shallow', 'coarse', 'displayed', 'received', 'planned', 'decommissioned', 'pleasant', 'illegal', 'biased', 'viewed', 'focused', 'canceled', 'exotic', 'weighted', 'dirty', 'elaborate', 'elegant', '##orted', 'mature', 'open', 'iphone', 'charging', 'passive', 'oriented', 'intended', 'deteriorated', 'played', 'violent', 'retired', 'precious', 'armed', 'buzzed', 'replace', 'electrical', 'laptop', 'corrupted', 'fielded', 'depleted', 'located', 'exposed', 'sited', 'maintenance', '##rous', 'manual', 'slender', 'monitored', 'noticeable', 'suspicious', 'fired', 'plastic', 'balanced', '##ggy', 'distracting', 'nice', 'hybrid', 'exported', 'inexperienced', 'contaminated', 'hairy', 'issued', 'protected', 'slim', 'big', 'pathetic', 'labelled', 'misleading', 'undeveloped', '##rdi', '##user', 'devices', 'trusted', '##verted', 'susceptible', 'quieter', 'coupled', 'logged', 'written', 'notebook', 'unmanned', 'vibrated', 'insufficient', 'unsigned', 'distinctive', 'pulled', 'power', 'moving', 'revised', 'frustrated', 'misunderstood', 'pricing', 'lethal', 'sleepy', 'virtual', 'excluded', 'hurried', 'agile', 'many', 'standardized', 'discouraged', 'working', 'service', 'dysfunction', 'billing', 'irrelevant', 'kept', 'styled', 'larger', 'often', 'fabricated', 'featured', 'original', 'cummings', 'stored', 'covered', 'installing', 'shielded', 'uneven', 'significant', 'failures', 'aimed', 'borrowed', 'tablet', 'ancient', 'abbreviated', 'mcdonnell', 'cute', '##fy', 'installation', 'brought', 'update', '##eased', 'degraded', 'energetic', 'implemented', 'delicious', 'mechanized', 'set', '##rdy', 'not', 'heavier', 'bypassed', 'spent', 'device', 'completed', 'operating', '2010s', 'huge', 'selective', 'recent', 'performed', 'precise', 'hybrids', 'brief', 'frozen', 'irregular', 'essential', 'updates', 'enchanted', 'selfish', 'increased', 'tome', 'use', 'pets', 'directional', 'tricky', 'transported', 'relaxed', 'multiplayer', 'mechanic', 'opened', 'crashed', 'cosmetic', 'transformed', '##υ', 'elderly', 'offered', 'numbered', '##loaded', 'branded', 'rebuilt', 'pregnant', 'competitors', 'potent', '##ecure', 'treated', 'detected', '##sty', 'lifespan', 'tough', 'using', 'replacement', 'hesitant', 'minimal', 'sneakers', 'unrelated', 'mundane', 'restrained', 'compared', 'omitted', 'satisfied', 'replacements', 'footprint', 'restored', 'ordinary', 'operational', 'armoured', 'predatory', 'added', 'recycled', 'digital', 'modest', 'arrogant', 'reactive', 'consumers', 'impatient', 'disliked', 'criticised', 'truncated', 'anonymous', 'fat', 'lacked', 'opaque', 'faded', 'wide', 'pretty', 'wrong', 'affluent', 'physical', 'scarred', 'false', 'crash', 'favored', 'recruited', 'reissued', 'scanned', 'buy', 'bothered', 'secured', 'secretive', 'curious', 'paid', 'impressed', 'mobility', '##ᅧ', 'independent', 'uses', 'firm', 'spaced', 'tilted', 'gross', 'metallic', 'painful', 'spoiled', 'peripheral', 'handicapped', 'superseded', 'eligible', 'mans', 'scaled', 'influenced', 'apps', 'confined', 'angled', 'hidden', '##mable', 'online', 'battery', 'enthusiast', 'futuristic', 'shy', 'retained', 'augmented', 'localized', 'upgrading', 'keyboard', 'patient', 'professional', 'processed', 'provided', 'convertible', 'signed', 'docked', 'addressed', 'conspicuous', 'finished', 'decorative', 'exclusive', 'torque', 'exploited', 'reserved', 'accessories', '##nostic', 'infected', '##powered', 'their', 'programmers', 'scrubbed', 'distracted', 'appealing', 'dynamic', 'deadly', 'organized', 'today', 'fined', 'sick', 'fragmented', 'constrained', 'shrill', 'erected', 'budget', 'challenging', '##sable', 'assigned', '##dgets', 'healthy', 'talented', 'mhz', 'optional', 'outgoing', 'dependent', 'hip', 'described', 'slippery', 'model', '##mute', 'rotated', 'shortened', 'dropped', 'dismissed', 'dimensional', 'modification', 'renamed', 'ported', 'spicy', 'involved', 'cleaner', 'obscure', 'ineligible', 'religious', 'legal', 'functioning', 'backward', 'bare', 'private', 'ipad', 'connectivity', 'blunt', 'streamlined', 'destroyed', 'loose', 'lacy', 'rolled', 'praised', 'the', 'faithful', '##band', 'trash', 'vague', 'unpleasant', 'dodged', '##tarian', 'cleaned', 'cursed', 'defined', 'impossible', 'served', 'fails', 'impoverished', 'incorrect', 'offensive', 'hummed', 'happy', 'branched', 'visible', 'preferred', 'cycling', 'uncertain', 'radioactive', 'controlled', 'disadvantaged', 'aromatic', '##tica', 'variant', '##ized', 'shelly', 'damned', 'fleeting', 'sealed', 'crooked', 'confiscated', 'overwhelmed', 'terminated', 'flex', 'housed', 'funky', 'foreign', 'nokia', 'prized', 'lighter', 'typical', 'salty', 'without', 'chip', '##placed', 'collected', 'feminine', 'stuffed', 'naughty', 'little', 'flaw', 'published', 'upscale', 'customer', 'detached', 'drawer', 'appointed', 'trivial', 'loyal', 'appropriate', 'adapt', 'dissatisfied', 'boxed', 'aware', 'junk', 'diminished', 'dell', 'limiting', 'engaging', '##ike', 'copying', 'mixed', 'retrieved', 'positioned', '##gent', 'saved', 'pointless', 'invalid', 'hardware', 'regular', 'dwarf', 'protective', 'void']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74CckPLHDmrT",
        "outputId": "dbc271cc-598c-41e2-fd36-b90ca403c51e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, v in enumerate(predicted_tokens):\n",
        "    if v in candidate:\n",
        "        print(i, v)\n",
        "        break"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74 complicated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO7K_6PyOZ6x"
      },
      "source": [
        "## 関数にしてみた"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kny1iT_eED6I"
      },
      "source": [
        "def part5_slover(text, candidate):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    masked_index = tokens.index(\"[MASK]\")\n",
        "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    ids = torch.tensor(ids).reshape(1,-1)\n",
        "    predictions = model(ids)[0][0]\n",
        "\n",
        "    _, predicted_indexes = torch.topk(predictions[masked_index+1], k=10000)\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes.tolist())\n",
        "\n",
        "    for i, v in enumerate(predicted_tokens):\n",
        "        if v in candidate:\n",
        "            return \"answer: \" + v\n",
        "    return \"don't know\""
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucJl0dG5JgG0"
      },
      "source": [
        "text = \"The supremarket giant donotes food to [MASK] people .\"\n",
        "candidate = [\"need\", \"needed\", \"needy\", \"necessary\"]\n",
        "#text = \"Although Mrs. Baker has U.S. citizenship, she is [MASK] from New Zealand .\"\n",
        "#candidate = [\"originally\", \"originality\", \"original\", \"originated\"]\n",
        "#text = \"The Chairperson shed false tears at the shareholders' meeting to [MASK] the dramatic effect .\"\n",
        "#candidate = [\"height\", \"heighten\", \"high\", \"highly\"]\n",
        "#text = \"Demonstrators gathrered in front of the pharmaceutical company to [MASK] against animal testing .\"\n",
        "#candidate = [\"prospect\", \"protect\", \"protest\", \"protract\"]\n",
        "#text = \"Miss Marting's argument for the new project was so [MASK] that no one could challenge it .\"\n",
        "#candidate = [\"persuasive\", \"persuading\", \"persuasion\", \"persuasively\"]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q_Vi027EGFL",
        "outputId": "9e74cbc4-b5bb-43a9-8875-450b64f44456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "part5_slover(text, candidate)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'answer: persuasion'"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snol5wIUawe"
      },
      "source": [
        "## 1. RoBERTa による文章中の空欄埋め\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ta1Zf-gUxJA"
      },
      "source": [
        "### モデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDMZTkNRRvkl"
      },
      "source": [
        "from transformers import T5Tokenizer, RobertaForMaskedLM\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-roberta-base\")\n",
        "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
        "\n",
        "model = RobertaForMaskedLM.from_pretrained(\"rinna/japanese-roberta-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYo4P-RpU1g3"
      },
      "source": [
        "### 問題文の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_y3KX3R9Fo"
      },
      "source": [
        "# original text\n",
        "text = \"4年に1度オリンピックは開かれる。\"\n",
        "#text = \"\"\n",
        "\n",
        "# prepend [CLS]\n",
        "text = \"[CLS]\" + text\n",
        "\n",
        "# tokenize\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# mask a token\n",
        "masked_idx = 5\n",
        "tokens[masked_idx] = tokenizer.mask_token\n",
        "print(tokens) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lao7fOXYU_df"
      },
      "source": [
        "### 穴埋め問題を解く"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XHqe2k4WPt8"
      },
      "source": [
        "補充すべき単語の推定 (id)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfCfwo7vSOmb"
      },
      "source": [
        "# convert to ids\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(token_ids)\n",
        "\n",
        "# convert to tensor\n",
        "import torch\n",
        "token_tensor = torch.LongTensor([token_ids])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s6nSS5YWZhy"
      },
      "source": [
        "結果の表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBBgnyLhSgpf"
      },
      "source": [
        "# provide position ids explicitly\n",
        "position_ids = list(range(0, token_tensor.size(1)))\n",
        "position_id_tensor = torch.LongTensor([position_ids])\n",
        "\n",
        "# get the top 10 predictions of the masked token\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=token_tensor, position_ids=position_id_tensor)\n",
        "    predictions = outputs[0][0, masked_idx].topk(10)\n",
        "\n",
        "for i, index_t in enumerate(predictions.indices):\n",
        "    index = index_t.item()\n",
        "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
        "    print(i, token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pla_3YxVN_g"
      },
      "source": [
        "## 2. GPT-2 による文書生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz2_A1zdkE3X"
      },
      "source": [
        "### (1) rinna/japanese-gpt2 の利用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THovObjOV18G"
      },
      "source": [
        "### モデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYr-6Sk4RXBb"
      },
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoa-OUqiWECt"
      },
      "source": [
        "### 文書生成の例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuj3d3wOE_cJ"
      },
      "source": [
        "input = tokenizer.encode(\"私が防衛大学校に入校してから、\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=3)\n",
        "sentences = tokenizer.batch_decode(output)\n",
        "for i in sentences:\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJNI3s4Dk0l_"
      },
      "source": [
        "ここで、train.txt と run_clm.py のアップロードを行う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEotsURZgNfq"
      },
      "source": [
        "%%time\n",
        "!rm -r output\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-small \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=10 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --output_dir=output/ \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXZQf15Sgeik"
      },
      "source": [
        "# モデルの準備\n",
        "model = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "\n",
        "# 推論\n",
        "input = tokenizer.encode(\"おはよう、お兄ちゃん。\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=8)\n",
        "sentences = tokenizer.batch_decode(output)\n",
        "for i in sentences:\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfnrGkVoj-5j"
      },
      "source": [
        "## (2) GPT2-Japanese の利用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1PcyX7Qku14"
      },
      "source": [
        "### モデルのダウンロードとインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvM-z3pdOZEi"
      },
      "source": [
        "# gpt2-japaneseのインストール\n",
        "!git clone https://github.com/tanreinama/gpt2-japanese\n",
        "%cd gpt2-japanese\n",
        "#!pip uninstall tensorflow -y\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC2ExWNabCeH"
      },
      "source": [
        "# smallモデルのダウンロード\n",
        "!wget https://www.nama.ne.jp/models/gpt2ja-small.tar.bz2\n",
        "!tar xvfj gpt2ja-small.tar.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAcoVLUgk5Rt"
      },
      "source": [
        "### ランダムな文章の作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDL1l6hdUNX"
      },
      "source": [
        "# smallモデルの動作確認\n",
        "!python gpt2-generate.py --model gpt2ja-small --num_generate 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lp-WPCTk1ph"
      },
      "source": [
        "### 文章の続きを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYG-XCyHebGa"
      },
      "source": [
        "!python gpt2-generate.py --model gpt2ja-small --num_generate 3 --context=\"私は防衛大学校に入校してから、\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaUhdN9recTk"
      },
      "source": [
        "# データセットの作成\n",
        "!git clone https://github.com/tanreinama/Japanese-BPEEncoder.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hg24DNNm0e4"
      },
      "source": [
        "ここで、gtp2-japanese の下に mydata というフォルダを作成し、データを置く。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuxnmlayoG-g"
      },
      "source": [
        "!python ./Japanese-BPEEncoder/encode_bpe.py --src_dir mydata --dst_file finetune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqCmn2uYwJ6K"
      },
      "source": [
        "ここで、run_finetune.py を gtp2-japanese の下に置く。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAwSANKXoPtU"
      },
      "source": [
        "!python run_finetune.py --base_model gpt2ja-small --dataset finetune.npz --run_name gpr2ja-finetune_run1-small"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL2paV3RppP2"
      },
      "source": [
        "!python gpt2-generate.py --model checkpoint/gpr2ja-finetune_run1-small --num_generate 8"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}